from pandas import read_csv
from os.path import join
from snakemake.utils import validate
from snakemake.utils import min_version
__author__ = "Graeme Ford"
__credits__ = [
    "Graeme Ford",
    "Prof. Michael S. Pepper",
    "Prof. Fourie Joubert",
    "Antionette Colic",
    "Fatima Barmania",
    "Megan Holborn",
]
__version__ = "1.0.0"
__maintainer__ = "Graeme Ford"
__email__ = "graeme.ford@tuks.co.za"
__status__ = "Development"

# Enforce version check
min_version("7.24.2")


# DEFINE CONFIG FILE FOR SNAKEMAKE:
configfile: join("config", "config.json")


# SET REPORT TEMPLATE
report: "report/template.rst"


# validate(config, join("..", "config", ".schema", "config.schema.json"))


# pepfile: join("config", "pep.yaml")


# pepschema: join("..", "config", ".schema", "pep.schema.yaml")

locations = read_csv(join("input", "locations.csv"), header=0)
samples = read_csv(join("input", "samples.csv"), header=0)
datasets = read_csv(join("input", "datasets.csv"), header=0)
transcripts = read_csv(join("input", "transcripts.csv"), header=0)


# DEFINE CONTEXT-VARIABLES:
# finalExtensions = ["acount", "hardy", "smiss", "vmiss"]  # "prune.in", "prune.out",
clusters = set([cluster for cluster in samples.keys() if cluster not in ["sample_name", "dataset"]])
# bExtensions = ["bed", "bim", "fam"]
# tExtensions = ["map", "ped"]


include: "rules/common.smk"

# [IMPORT] VCF-Validation-Workflow and override local rules with non-local input from theVCF-Validation-Workflow
include: "rules/importVcfValidationWorkflow.smk"

# [IMPORT] Population-Structure-Workflow and override local rules with non-local input from the Population-Structure-Workflow
# include: "rules/importPopulationStructureWorkflow.smk"

# DEFINE CONTAINERIZED ENVIRONMENT:
container: "docker://graemeford/pipeline-os"

rule mergeDatasets:
    """
    This rule merges multiple datasets into one large psudo-dataset that can be worked on more easily.
    """
    # group: "COLLATE"
    log: "results/tmp/mergeDatasets/All_mergeDatasets.log"
    benchmark: "results/tmp/mergeDatasets/All_mergeDatasets.benchmark"
    resources:
        cpus=search("cores", "mergeDatasets"),
        nodes=search("nodes", "mergeDatasets"),
        queue=search("queue", "mergeDatasets"),
        walltime=search("walltime", "mergeDatasets"),
    envmodules:
        config["environment"]["envmodules"]["bcftools"]
    input:
        vcf=lambda _: vcfValidationWorkflowAdapter(".vcf.gz"),
        vcfIndex=lambda _: vcfValidationWorkflowAdapter(".vcf.gz.tbi"),
    output:
        "results/tmp/mergeDatasets/All_mergeDatasets.vcf.gz"
    shell:
        """
        bcftools merge -O z -o {output} {input.vcf}
        """

rule refFromFasta:
    """
    This rule verifies that the reference alleles in teh provided VCF file match that of the reference genome.
    """
    # group: "refFromFasta"
    log: "results/tmp/refFromFasta/All_refFromFasta.log"
    benchmark: "results/tmp/refFromFasta/All_refFromFasta.benchmark"
    resources:
        cpus=search("cores", "refFromFasta"),
        nodes=search("nodes", "refFromFasta"),
        queue=search("queue", "refFromFasta"),
        walltime=search("walltime", "refFromFasta"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"]
    params:
        ref=lambda wildcards: join(
        *next(
            i["file_path"]
            for i in config["reference-genomes"]
        if i["version"] == "GRCh38"
            ),
        ),
        sample_sex_assignments=lambda wildcards: samples.columns.get_loc("sex")
    input: 
        vcf=lambda wildcards: "results/tmp/mergeDatasets/All_mergeDatasets.vcf.gz" if datasets.shape[0] >1 else vcfValidationWorkflowAdapter(".vcf.gz"),
        vcfIndex=lambda wildcards: "results/tmp/mergeDatasets/All_mergeDatasets.vcf.gz.tbi" if datasets.shape[0] >1 else vcfValidationWorkflowAdapter(".vcf.gz.tbi"),
        sampleSexes="input/samples.csv"
    output: 
        "results/tmp/refFromFasta/All_refFromFasta.vcf.gz"
    shell:
        """
        plink2 --vcf {input.vcf} --fa {params.ref} --ref-from-fa force --allow-extra-chr --update-sex {params.sample_sex_assignments} col-num=2 --split-par hg38 --export vcf-4.2 bgz --out results/COLLATE/refFromFasta
        """

rule chrFilter:
    """
    This rule removes any unusual chromosomes.
    """
    # group: "FILTER"
    log: "results/tmp/chrFilter/All_chrFilter.log"
    benchmark: "results/tmp/chrFilter/All_chrFilter.benchmark"
    resources:
        cpus=search("cores", "chrFilter"),
        nodes=search("nodes", "chrFilter"),
        queue=search("queue", "chrFilter"),
        walltime=search("walltime", "chrFilter"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"]
    input:
        vcf="results/tmp/refFromFasta/All_refFromFasta.vcf.gz",
        vcfIndex="results/tmp/refFromFasta/All_refFromFasta.vcf.gz.tbi"
    output:
        "results/tmp/chrFilter/All_chrFilter.vcf.gz"
    shell:
        """
        plink2 --vcf {input.vcf} --allow-extra-chr --output-chr chr26 --chr 1-26 --export vcf-4.2 bgz --out results/COLLATE/chrFilter
        """

rule filterRequestedSamples:
    """
    This rule subsets samples according to user defined list and remove variants that do not pass QC.
    """
    # group: "FILTER"
    log: "results/tmp/filterRequestedSamples/All_filterRequestedSamples.log"
    benchmark: "results/tmp/filterRequestedSamples/All_filterRequestedSamples.benchmark"
    resources:
        cpus=search("cores", "filterRequestedSamples"),
        nodes=search("nodes", "filterRequestedSamples"),
        queue=search("queue", "filterRequestedSamples"),
        walltime=search("walltime", "filterRequestedSamples")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    params:
        samples=lambda wildcards, input: ",".join(samples["sample_name"].tolist())
    input:
        vcf="results/tmp/chrFilter/All_chrFilter.vcf.gz",
        samples="input/samples.csv"
    output:
        "results/tmp/filterRequestedSamples/All_filterRequestedSamples.vcf.gz"
    shell:
        """
        bcftools view -s {params.samples} -O z -o {output} {input.vcf}  2>{log}
        """



rule filterVariantMissingness:
    """
    Filter out variants with >= 100% missingness
    """

    log: "results/tmp/filterVariantMissingness/All_filterVariantMissingness.log"
    benchmark: "results/tmp/filterVariantMissingness/All_filterVariantMissingness.benchmark"
    resources: 
        cpus=search("cores", "filterVariantMissingness"),
        nodes=search("nodes", "filterVariantMissingness"),
        queue=search("queue", "filterVariantMissingness"),
        walltime=search("walltime", "filterVariantMissingness"), 
    envmodules:
        config["environment"]["envmodules"]["plink-2"],
    params:
        output=lambda wildcards, output: output[0][:-7]
    input: 
        "results/tmp/filterRequestedSamples/All_filterRequestedSamples.vcf.gz"
    output:
        "results/tmp/filterVariantMissingness/All_filterVariantMissingness.vcf.gz"
    shell: 
        """
        echo -e "\n--- LOG SECTION START | Plink-2 'Filter 100% Missingness variants' ---" 1>&2
        plink2 --chr 1-26 --allow-extra-chr --vcf {input} --geno 1.0 --output-chr chr26 --export vcf-4.2 bgz --out {params.output}
        echo -e "--- LOG SECTION END | Plink-2 'Filter 100% Missingness variants' ---\n" 1>&2
        """

rule filterSampleMissingness:
    """
    Filter out samples with >= 100% missingness
    """

    log: "results/tmp/filterSampleMissingness/All_filterSampleMissingness.log"
    benchmark: "results/tmp/filterSampleMissingness/All_filterSampleMissingness.benchmark"
    resources: 
        cpus=search("cores", "filterSampleMissingness"),
        nodes=search("nodes", "filterSampleMissingness"),
        queue=search("queue", "filterSampleMissingness"),
        walltime=search("walltime", "filterSampleMissingness"), 
    envmodules:
        config["environment"]["envmodules"]["plink-2"],
    params:
        output=lambda wildcards, output: output[0][:-7]
    input:
        "results/tmp/filterVariantMissingness/All_filterVariantMissingness.vcf.gz"
    output:
        "results/tmp/filterSampleMissingness/All_filterSampleMissingness.vcf.gz"
    shell: 
        """
        echo -e "\n--- LOG SECTION START | Plink-2 'Filter 100% Missingness samples' ---" 1>&2
        plink2 --chr 1-26 --allow-extra-chr --vcf {input} --mind 1.0 --output-chr chr26 --export vcf-4.2 bgz --out {params.output}
        echo -e "--- LOG SECTION END | Plink-2 'Filter 100% Missingness samples' ---\n" 1>&2
        """

rule calculateLinkageDisequilibrium:
    """
    Identify variants in linkage disequilibrium
    """

    log: "results/tmp/calculateLinkageDisequilibrium/All_calculateLinkageDisequilibrium.log"
    benchmark: "results/tmp/calculateLinkageDisequilibrium/All_calculateLinkageDisequilibrium.benchmark"
    resources:
        cpus=search("cores", "calculateLinkageDisequilibrium"),
        nodes=search("nodes", "calculateLinkageDisequilibrium"),
        queue=search("queue", "calculateLinkageDisequilibrium"),
        walltime=search("walltime", "calculateLinkageDisequilibrium"),
    envmodules: 
        config["environment"]["envmodules"]["plink-2"],
    params:
        output=lambda wildcards, output: output[0][:-9]
    input:
        "results/tmp/filterSampleMissingness/All_filterSampleMissingness.vcf.gz",
    output:
        "results/tmp/calculateLinkageDisequilibrium/All_calculateLinkageDisequilibrium.prune.in",
        "results/tmp/calculateLinkageDisequilibrium/All_calculateLinkageDisequilibrium.prune.out",
    shell:
        """
        plink2 --vcf {input} --chr 1-26 --new-id-max-allele-len 1000 --rm-dup exclude-mismatch --indep-pairwise 50 5 0.5 --bad-ld --out {params.output}
        """


rule filterLinkageDisequilibrium:
    """
    """

    log: "results/tmp/filterLinkageDisequilibrium/All_filterLinkageDisequilibrium.log"
    benchmark: "results/tmp/filterLinkageDisequilibrium/All_filterLinkageDisequilibrium.benchmark"
    resources:
        cpus=search("cores", "filterLinkageDisequilibrium"),
        nodes=search("nodes", "filterLinkageDisequilibrium"),
        queue=search("queue", "filterLinkageDisequilibrium"),
        walltime=search("walltime", "filterLinkageDisequilibrium"),
    envmodules: 
        config["environment"]["envmodules"]["plink-2"],
    params:
        output=lambda wildcards, output: output[0][:-7]
    input:
        inclusion_list="results/tmp/calculateLinkageDisequilibrium/All_calculateLinkageDisequilibrium.prune.in",
        vcf="results/tmp/filterSampleMissingness/All_filterSampleMissingness.vcf.gz"
    output:
        "results/tmp/filterLinkageDisequilibrium/All_filterLinkageDisequilibrium.vcf.gz"
    shell:
        """
        plink2 --allow-extra-chr --vcf {input.vcf} --extract {input.inclusion_list} --export vcf-4.2 bgz --out {params.output}
        """

# TODO: Remove reliance on --recode-iid and use *.genome text-based output
# TODO: Write bridging script to extract list of sample-ids from *.genome text format using Pandas
rule calculateSampleRelatedness:
    """
    Identify samples which are related to eachother using Identity-By-Descent
    """

    log: "results/tmp/calculateIdentityByDescent/All_calculateIdentityByDescent.log"
    benchmark: "results/tmp/calculateIdentityByDescent/All_calculateIdentityByDescent.benchmark"
    resources:
        cpus=search("cores", "calculateIdentityByDescent"),
        nodes=search("nodes", "calculateIdentityByDescent"),
        queue=search("queue", "calculateIdentityByDescent"),
        walltime=search("walltime", "calculateIdentityByDescent"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"],
    params:
        output=lambda wildcards, output: output["inclusion_list"][:-18]
    input:
        "results/tmp/filterLinkageDisequilibrium/All_filterLinkageDisequilibrium.vcf.gz"
    output:
        inclusion_list="results/tmp/calculate/calculateIdentityByDescent.king.cutoff.in.id",
        exclusion_list="results/tmp/calculate/calculateIdentityByDescent.king.cutoff.out.id"
    shell:
        """
        plink2 --vcf {input} --allow-extra-chr --king-cutoff 0.354 --out {params.output}
        """


rule filterSampleRelatedness:
    """
    Subset the data by these unrelated individuals
    """
    # group: "FILTER"
    log: "results/tmp/filterSampleRelatedness/All_SampleRelatedness.log"
    benchmark: "results/tmp/filterSampleRelatedness/All_SampleRelatedness.benchmark"
    resources:
        cpus=search("cores", "filterSampleRelatedness"),
        nodes=search("nodes", "filterSampleRelatedness"),
        queue=search("queue", "filterSampleRelatedness"),
        walltime=search("walltime", "filterSampleRelatedness"),
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    params:
        out=lambda wildcards, output: output[0][:-7]
    input:
        vcf="results/tmp/filterLinkageDisequilibrium/All_filterLinkageDisequilibrium.vcf.gz",
        unrelated_samples="results/tmp/calculate/calculateIdentityByDescent.king.cutoff.in.id"
    output:
        "results/tmp/filterSampleRelatedness/All_filterSampleRelatedness.vcf.gz",
    shell:
        """
        echo -e "\n--- LOG SECTION START | Plink-2 '--keep' ---" 1>&2
        plink2 --vcf {input.vcf} --split-par b38  --keep {input.unrelated_samples} --export vcf-4.2 bgz --out {params.out}
        echo -e "--- LOG SECTION END | Plink-2 '--keep' ---\n" 1>&2
        """


rule filterLocations:
    """
    Trim the whole-genome psudo-datasets down to several regions of interest for Variant analysis and Variant effect prediction.
    """
    # group: "FILTER"
    log: "results/tmp/filterLocations/{location}_filterLocations.log",
    benchmark: "results/tmp/filterLocations/{location}_filterLocations.benchmark"
    resources:
        cpus=search("cores", "filterLocations"),
        nodes=search("nodes", "filterLocations"),
        queue=search("queue", "filterLocations"),
        walltime=search("walltime", "filterLocations"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"],
    params:
        fromBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "start"].item(),
        toBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "stop"].item(),
        chr=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item(),
        out=lambda wildcards, output: output["vcf"][:-7]
    input:
        vcf="results/tmp/filterSampleRelatedness/All_filterSampleRelatedness.vcf.gz",
    output:
        vcf="results/tmp/filterLocations/{location}_filterLocations.vcf.gz",
    shell:
        """
        echo -e "\n--- LOG SECTION START | Plink-2 'trim' ---" 1>&2
        plink2 --vcf {input} --allow-extra-chr --from-bp {params.fromBP} --to-bp {params.toBP} --chr {params.chr} --output-chr chr26 --export vcf-4.2 bgz --out {params.out}
        echo -e "--- LOG SECTION END | Plink-2 'trim' ---\n" 1>&2
        """



rule writeSampleMetadata:
    """
    Convert Cluster information given in the config file into PLINK-2.0 suitable format.
    """
    # group: "FILTER"
    log: "results/tmp/writeSampleMetadata/{cluster}_writeSampleMetadata.log",
    benchmark: "results/tmp/writeSampleMetadata/{cluster}_writeSampleMetadata.benchmark"
    resources:
        cpus=search("cores", "writeSampleMetadata"),
        nodes=search("nodes", "writeSampleMetadata"),
        queue=search("queue", "writeSampleMetadata"),
        walltime=search("walltime", "writeSampleMetadata"),
    conda:
        join("envs", "snakemake.yml")
    envmodules:
        config["environment"]["envmodules"]["python-3"]
    output:
        metadata="results/tmp/writeSampleMetadata/{cluster}_writeSampleMetadata.txt",
    script:
        join("scripts", "01-TRANSPILE_CLUSTERS.py")


# rule reportFreq:
#     """
#     Perform Frequency analysis on super populations.
#     """
#     # group: "REPORT"
#     log: "logs/PLINK/{location}.log",
#     benchmark: "_benchmarks/reportFreq/{location}.benchmark"
#     resources:
#         cpus=search("cores", "reportFreq"),
#         nodes=search("nodes", "reportFreq"),
#         queue=search("queue", "reportFreq"),
#         walltime=search("walltime", "reportFreq"),
#     envmodules:
#         config["environment"]["envmodules"]["plink-2"],
#     params:
#         prefix="ALL_{location}",
#         subsets_list=lambda wildcards: ' '.join(clusters)
#     input:
#         [f"results/tmp/writeSampleMetadata/{cluster}_writeSampleMetadata.txt" for cluster in clusters],
#         vcf="results/tmp/filterLocations/{location}_filterLocations.vcf.gz",
#     output:
#         [
#             expand(
#                 [
#                     "results/FINAL/%s/ALL_{{location}}.%s.{extension}"
#                     % (cluster, population)
#                     for population in list(samples[cluster].unique())
#                 ],
#                 extension=finalExtensions,
#             )
#             for cluster in clusters
#         ],
#     shell:
#         """
#         for CLUSTER in {params.subsets_list}
#         do
#             echo -e "\n--- LOG SECTION START | Plink-2 'All freq' ---" 1>&2
#             plink2 --allow-extra-chr --vcf {input.vcf} --freq counts --export vcf-4.2 bgz --out results/FINAL/$CLUSTER/{params.prefix}
#             echo -e "--- LOG SECTION END | Plink-2 'All freq' ---\n" 1>&2

#             echo -e "\n--- LOG SECTION START | Plink-2 'Subset freq' ---" 1>&2
#             plink2 --allow-extra-chr --vcf {input.vcf} --pheno iid-only results/REFERENCE/cluster_$CLUSTER.txt --loop-cats $CLUSTER --freq counts --missing --hardy midp --out results/FINAL/$CLUSTER/{params.prefix}
#             echo -e "--- LOG SECTION END | Plink-2 'Subset freq' ---\n" 1>&2
#         done
#         """

rule reportFreqAllPerLocation:
    """
    Perform Frequency analysis on super populations.
    """
    # group: "REPORT"
    log: "logs/PLINK/{location}.log",
    benchmark: "_benchmarks/reportFreq/{location}.benchmark"
    resources:
        cpus=search("cores", "reportFreq"),
        nodes=search("nodes", "reportFreq"),
        queue=search("queue", "reportFreq"),
        walltime=search("walltime", "reportFreq"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"],
    params:
        out=lambda wildcards,output: output["allele_count"][:-7],
    input:
        vcf="results/tmp/filterLocations/{location}_filterLocations.vcf.gz",
    output:
        allele_count="results/reportFreqAllPerLocation/All_{location}.acount",
        hardy="results/reportFreqAllPerLocation/All_{location}.hardy",
        sample_missingness="results/reportFreqAllPerLocation/All_{location}.smiss",
        variant_missingness="results/reportFreqAllPerLocation/All_{location}.vmiss",
    shell:
        """
        echo -e "\n--- LOG SECTION START | Plink-2 'All freq' ---" 1>&2
        plink2 --vcf {input.vcf} --allow-extra-chr --split-par b38 --freq counts --export vcf-4.2 bgz --out {params.out}
        echo -e "--- LOG SECTION END | Plink-2 'All freq' ---\n" 1>&2
        """

rule reportFreqPerClusterPerLocation:
    """
    Perform Frequency analysis on super populations.
    """
    # group: "REPORT"
    log: "logs/PLINK/{cluster}_{location}.log",
    benchmark: "_benchmarks/reportFreq/{cluster}_{location}.benchmark"
    resources:
        cpus=search("cores", "reportFreq"),
        nodes=search("nodes", "reportFreq"),
        queue=search("queue", "reportFreq"),
        walltime=search("walltime", "reportFreq"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"],
    params:
        out=lambda wildcards,output: output["allele_count"][:-7]
    input:
        vcf="results/tmp/filterLocations/{location}_filterLocations.vcf.gz",
        cluster_samples="results/tmp/writeSampleMetadata/{cluster}_writeSampleMetadata.txt"
    output:
        allele_count="results/reportFreqPerClusterPerLocation/{cluster}_{location}.acount",
        hardy="results/reportFreqPerClusterPerLocation/{cluster}_{location}.hardy",
        sample_missingness="results/reportFreqPerClusterPerLocation/{cluster}_{location}.smiss",
        variant_missingness="results/reportFreqPerClusterPerLocation/{cluster}_{location}.vmiss",
    shell:
        """
        echo -e "\n--- LOG SECTION START | Plink-2 'Subset freq' ---" 1>&2
        plink2 --vcf {input.vcf} --allow-extra-chr --keep {input.cluster_samples} --freq counts --missing --hardy midp --out {params.out}
        echo -e "--- LOG SECTION END | Plink-2 'Subset freq' ---\n" 1>&2
        """

# group: VALIDATE
rule tabix:
    """
    Generate tabix-index.
    """
    log: "results/tmp/{operation}/{output}_{operation}.log"
    benchmark: "results/tmp/{operation}/{output}_{operation}.benchmark"
    resources:
        cpus=search("cores", "tabix"),
        nodes=search("nodes", "tabix"),
        queue=search("queue", "tabix"),
        walltime=search("walltime", "tabix")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    input:
        "results/tmp/{operation}/{output}_{operation}.vcf.gz"
    output:
        "results/tmp/{operation}/{output}_{operation}.vcf.gz.tbi"
    shell:
        """
        tabix -p vcf {input}
        """

rule all:
    """
    Catch-all rule to trigger auto-run of all processes. This process will be fired automatically in absence of explicit process name given by cli-argument.
    """
    default_target: True
    log:
        "logs/ALL/ALL.log",
    input:
        VCFValidation.rules.all.input,
        expand("results/reportFreqPerClusterPerLocation/{cluster}_{location}.acount", cluster=clusters, location=locations["location_name"]),
        expand("results/reportFreqPerClusterPerLocation/{cluster}_{location}.hardy", cluster=clusters, location=locations["location_name"]),
        expand("results/reportFreqPerClusterPerLocation/{cluster}_{location}.smiss", cluster=clusters, location=locations["location_name"]),
        expand("results/reportFreqPerClusterPerLocation/{cluster}_{location}.vmiss", cluster=clusters, location=locations["location_name"]),
        expand("results/reportFreqAllPerLocation/All_{location}.acount", location=locations["location_name"]),
        expand("results/reportFreqAllPerLocation/All_{location}.hardy", location=locations["location_name"]),
        expand("results/reportFreqAllPerLocation/All_{location}.smiss", location=locations["location_name"]),
        expand("results/reportFreqAllPerLocation/All_{location}.vmiss", location=locations["location_name"]),
        # PopulationStructure.rules.all.input,
