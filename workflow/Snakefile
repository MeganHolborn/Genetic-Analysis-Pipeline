from pandas import read_csv, Series
from os.path import join
from snakemake.utils import validate
from snakemake.utils import min_version
__author__ = "Graeme Ford"
__credits__ = [
    "Graeme Ford",
    "Prof. Michael S. Pepper",
    "Prof. Fourie Joubert",
    "Fatima Barmania",
    "Megan Holborn",
]
__version__ = "1.0.0"
__maintainer__ = "Graeme Ford"
__email__ = "graeme.ford@tuks.co.za"
__status__ = "Development"

# Enforce version check
min_version("7.24.2")


# DEFINE CONFIG FILE FOR SNAKEMAKE:
configfile: join("config", "config.json")


# SET REPORT TEMPLATE
report: "report/template.rst"


# validate(config, join("..", "config", ".schema", "config.schema.json"))


# pepfile: join("config", "pep.yaml")


# pepschema: join("..", "config", ".schema", "pep.schema.yaml")

locations = read_csv(join("input", "locations.csv"), header=0)
samples = read_csv(join("input", "samples.csv"), header=0)
datasets = read_csv(join("input", "datasets.csv"), header=0)
transcripts = read_csv(join("input", "transcripts.csv"), header=0)


# DEFINE CONTEXT-VARIABLES:
# finalExtensions = ["acount", "hardy", "smiss", "vmiss"]  # "prune.in", "prune.out",
clusters = set([cluster for cluster in samples.keys() if cluster not in ["sample_name", "dataset"]])
# bExtensions = ["bed", "bim", "fam"]
# tExtensions = ["map", "ped"]


include: "rules/common.smk"

# [IMPORT] VCF-Validation-Workflow and override local rules with non-local input from theVCF-Validation-Workflow
include: "rules/importVcfValidationWorkflow.smk"

# [IMPORT] Custom functions to connect checkpoints with variable outputs
include: "rules/checkpoint_connectors.smk"


# [IMPORT] Population-Structure-Workflow and override local rules with non-local input from the Population-Structure-Workflow
# include: "rules/importPopulationStructureWorkflow.smk"

# DEFINE CONTAINERIZED ENVIRONMENT:
container: "docker://graemeford/pipeline-os"


rule format_sample_metadata:
    log: outputDir("tmp/formatted_sample_metadata/All.log")
    benchmark: outputDir("tmp/formatted_sample_metadata/All.benchmark")
    conda:
        join("envs", "snakemake.yml")
    input:
        sample_annotations="input/samples.csv"
    output:
        sample_metadata=outputDir("tmp/formatted_sample_metadata/samples.tsv")
    script:
        join("scripts", "01.5-TRANSPILE_SAMPLE_SEXES.py")


rule tabix:
    log: outputDir("tmp/{output}/{operation}.log")
    benchmark: outputDir("tmp/{output}/{operation}.benchmark")
    wildcard_constraints:
        output=r"[a-zA-Z0-9\-]+",
        operation=r"[a-zA-Z0-9\-\_]+"
    input:
        outputDir("tmp/{output}/{operation}.vcf.gz")
    output:
        outputDir("tmp/{output}/{operation}.vcf.gz.tbi")
    shell:
        """
        tabix -p vcf {input}
        """


rule merge_datasets:
    log: outputDir("tmp/{contig}/merged_datasets.log")
    benchmark: outputDir("tmp/{contig}/merged_datasets.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}",
    input:
        # vcf=lambda wildcards: vcfValidationWorkflowAdapter(".vcf.gz", wildcards),
        vcf=expand(outputDir("tmp/{{contig}}/{dataset}/sorted_variant_records.vcf.gz"), dataset=datasets["dataset_name"].unique().tolist()),
        # vcfIndex=lambda wildcards: vcfValidationWorkflowAdapter(".vcf.gz.tbi", wildcards),
        vcfIndex=expand(outputDir("tmp/{{contig}}/{dataset}/sorted_variant_records.vcf.gz.tbi"), dataset=datasets["dataset_name"].unique().tolist()),
    output:
         outputDir("tmp/{contig}/merged_datasets.vcf.gz")
    shell:
        """
        bcftools merge -O z -o {output} {input.vcf}
        """


rule normalize_merged_datasets:
    log: outputDir("tmp/{contig}/normalized_merged_datasets.log")
    benchmark: outputDir("tmp/{contig}/normalized_merged_datasets.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    input:
        outputDir("tmp/{contig}/merged_datasets.vcf.gz")
    output:
        outputDir("tmp/{contig}/normalized_merged_datasets.vcf.gz")
    shell:
        """
        bcftools norm --multiallelics -any -Oz -o {output} {input}
        """


rule convert_to_pgen:
    log: outputDir("tmp/{contig}/converted_to_pgen.log")
    benchmark: outputDir("tmp/{contig}/converted_to_pgen.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        output=lambda wildcards, output: output["pgen"].replace(".pgen", "")
    input: 
        vcf=lambda wildcards: outputDir("tmp/{contig}/normalized_merged_datasets.vcf.gz") if datasets.shape[0] >1 else vcfValidationWorkflowAdapter(".vcf.gz"),
        vcfIndex=lambda wildcards: outputDir("tmp/{contig}/merged_datasets.vcf.gz.tbi") if datasets.shape[0] >1 else vcfValidationWorkflowAdapter(".vcf.gz.tbi"),
        sample_metadata=outputDir("tmp/formatted_sample_metadata/samples.tsv")
    output: 
        pgen=outputDir("tmp/{contig}/converted_to_pgen.pgen"),
        pvar=outputDir("tmp/{contig}/converted_to_pgen.pvar.zst"),
        psam=outputDir("tmp/{contig}/converted_to_pgen.psam"),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --vcf {input.vcf} --update-sex {input.sample_metadata} --split-par hg38 --allow-extra-chr --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule verify_records_against_reference_genome:
    log: outputDir("tmp/{contig}/verified_records_against_reference_genome.log")
    benchmark: outputDir("tmp/{contig}/verified_records_against_reference_genome.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards, output: output["pgen"].replace(".pgen", ""),
        ref=lambda wildcards: join(
        *next(
            i["file_path"]
            for i in config["reference-genomes"]
        if i["version"] == "GRCh38"
            ),
        )
    input: 
        pgen=outputDir("tmp/{contig}/converted_to_pgen.pgen"),
        pvar=outputDir("tmp/{contig}/converted_to_pgen.pvar.zst"),
        psam=outputDir("tmp/{contig}/converted_to_pgen.psam")
    output: 
        pgen=outputDir("tmp/{contig}/verified_records_against_reference_genome.pgen"),
        pvar=outputDir("tmp/{contig}/verified_records_against_reference_genome.pvar.zst"),
        psam=outputDir("tmp/{contig}/verified_records_against_reference_genome.psam"),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --fa {params.ref} --ref-from-fa force --allow-extra-chr --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule remove_non_standard_chromosomes:
    log: outputDir("tmp/{contig}/removed_non_standard_chromosomes.log")
    benchmark: outputDir("tmp/{contig}/removed_non_standard_chromosomes.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards,output: output["pgen"].replace(".pgen", "")
    input:
        pgen=outputDir("tmp/{contig}/verified_records_against_reference_genome.pgen"),
        pvar=outputDir("tmp/{contig}/verified_records_against_reference_genome.pvar.zst"),
        psam=outputDir("tmp/{contig}/verified_records_against_reference_genome.psam"),
    output:
        pgen=outputDir("tmp/{contig}/removed_non_standard_chromosomes.pgen"),
        pvar=outputDir("tmp/{contig}/removed_non_standard_chromosomes.pvar.zst"),
        psam=outputDir("tmp/{contig}/removed_non_standard_chromosomes.psam")
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --allow-extra-chr --output-chr chr26 --chr 1-26 --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule remove_unknown_samples:
    log: outputDir("tmp/{contig}/removed_unknown_samples.log")
    benchmark: outputDir("tmp/{contig}/removed_unknown_samples.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards, output: output["pgen"].replace(".pgen", ""),
        samples=lambda wildcards, input: ",".join(samples["sample_name"].tolist())
    input:
        pgen=outputDir("tmp/{contig}/removed_non_standard_chromosomes.pgen"),
        pvar=outputDir("tmp/{contig}/removed_non_standard_chromosomes.pvar.zst"),
        psam=outputDir("tmp/{contig}/removed_non_standard_chromosomes.psam"),
        sample_metadata=outputDir("tmp/formatted_sample_metadata/samples.tsv")
    output:
        pgen=outputDir("tmp/{contig}/removed_unknown_samples.pgen"),
        pvar=outputDir("tmp/{contig}/removed_unknown_samples.pvar.zst"),
        psam=outputDir("tmp/{contig}/removed_unknown_samples.psam"),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --keep {input.sample_metadata} --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule filter_variant_missingness:
    log: outputDir("tmp/{contig}/filtered_variant_missingness.log")
    benchmark: outputDir("tmp/{contig}/filtered_variant_missingness.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards, output: output["pgen"].replace(".pgen", "")
    input: 
        pgen=outputDir("tmp/{contig}/removed_unknown_samples.pgen"),
        pvar=outputDir("tmp/{contig}/removed_unknown_samples.pvar.zst"),
        psam=outputDir("tmp/{contig}/removed_unknown_samples.psam"),
    output:
        pgen=outputDir("tmp/{contig}/filtered_variant_missingness.pgen"),
        pvar=outputDir("tmp/{contig}/filtered_variant_missingness.pvar.zst"),
        psam=outputDir("tmp/{contig}/filtered_variant_missingness.psam"),
    threads: workflow.cores * 0.25
    shell: 
        """
        plink2 --threads {threads} --pfile {params.input} vzs  --geno 1.0 --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule filter_sample_missingness:
    log: outputDir("tmp/{contig}/filtered_sample_missingness.log")
    benchmark: outputDir("tmp/{contig}/filtered_sample_missingness.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards, output: output["pgen"].replace(".pgen", "")
    input:
        pgen=outputDir("tmp/{contig}/filtered_variant_missingness.pgen"),
        pvar=outputDir("tmp/{contig}/filtered_variant_missingness.pvar.zst"),
        psam=outputDir("tmp/{contig}/filtered_variant_missingness.psam"),
    output:
        pgen=outputDir("tmp/{contig}/filtered_sample_missingness.pgen"),
        pvar=outputDir("tmp/{contig}/filtered_sample_missingness.pvar.zst"),
        psam=outputDir("tmp/{contig}/filtered_sample_missingness.psam"),
    threads: workflow.cores * 0.25
    shell: 
        """
        plink2 --threads {threads} --pfile {params.input} vzs --mind 1.0 --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule calculate_sample_relatedness:
    log: outputDir("tmp/{contig}/calculated_sample_relatedness.log")
    benchmark: outputDir("tmp/{contig}/calculated_sample_relatedness.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards, output: output["inclusion_list"].replace(".king.cutoff.in.id", "")
    input:
        pgen=outputDir("tmp/{contig}/filtered_sample_missingness.pgen"),
        pvar=outputDir("tmp/{contig}/filtered_sample_missingness.pvar.zst"),
        psam=outputDir("tmp/{contig}/filtered_sample_missingness.psam"),
    output:
        inclusion_list=outputDir("tmp/{contig}/calculated_sample_relatedness.king.cutoff.in.id"),
        exclusion_list=outputDir("tmp/{contig}/calculated_sample_relatedness.king.cutoff.out.id")
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --king-cutoff 0.354 --out {params.output} >{log} 2>&1
        """


rule remove_related_samples:
    log: outputDir("tmp/{contig}/removed_related_samples.log")
    benchmark: outputDir("tmp/{contig}/removed_related_samples.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards, output: output["pgen"].replace(".pgen", "")
    input:
        pgen=outputDir("tmp/{contig}/filtered_sample_missingness.pgen"),
        pvar=outputDir("tmp/{contig}/filtered_sample_missingness.pvar.zst"),
        psam=outputDir("tmp/{contig}/filtered_sample_missingness.psam"),
        unrelated_samples=outputDir("tmp/{contig}/calculated_sample_relatedness.king.cutoff.in.id"),
    output:
        pgen=outputDir("tmp/{contig}/removed_related_samples.pgen"),
        pvar=outputDir("tmp/{contig}/removed_related_samples.pvar.zst"),
        psam=outputDir("tmp/{contig}/removed_related_samples.psam"),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --keep {input.unrelated_samples} --make-pgen vzs --out {params.output} >{log} 2>&1
        """


rule remove_rare_variants:
    log: outputDir("tmp/{contig}/removed_rare_variants.log"),
    benchmark: outputDir("tmp/{contig}/removed_rare_variants.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        input=lambda wildcards, input: input["pgen"].replace('.pgen', ""),
        output=lambda wildcards, output: output["pgen"].replace(".pgen", ""),
    input:
        pgen=outputDir("tmp/{contig}/removed_related_samples.pgen"),
        pvar=outputDir("tmp/{contig}/removed_related_samples.pvar.zst"),
        psam=outputDir("tmp/{contig}/removed_related_samples.psam"),
        sample_metadata=outputDir("tmp/formatted_sample_metadata/samples.tsv")
    output:
        pgen=outputDir("tmp/{contig}/removed_rare_variants.pgen"),
        pvar=outputDir("tmp/{contig}/removed_rare_variants.pvar.zst"),
        psam=outputDir("tmp/{contig}/removed_rare_variants.psam"),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --pheno {input.sample_metadata} --maf 0.01 --make-pgen vzs --out {params.output} >{log} 2>&1
        """   


checkpoint calculate_linkage_disequilibrium_per_cluster:
    log: outputDir("linkage_disequilibrium/{cluster}/{location}/calculated_linkage_disequilibrium_per_cluster.log")
    benchmark: outputDir("linkage_disequilibrium/{cluster}/{location}/calculated_linkage_disequilibrium_per_cluster.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        fromBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "ld_start"].item(),
        toBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "ld_stop"].item(),
        chr=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item(),
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards: outputDir(f"linkage_disequilibrium/{wildcards.cluster}/{wildcards.location}/calculated_linkage_disequilibrium_per_cluster")
    wildcard_constraints:
        cluster=r"[a-zA-Z0-9\-]+",
        location=r"[a-zA-Z0-9\-]+"
    input:
        pgen=lambda wildcards: outputDir(f"tmp/{locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item()}/removed_rare_variants.pgen"),
        pvar=lambda wildcards: outputDir(f"tmp/{locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item()}/removed_rare_variants.pvar.zst"),
        psam=lambda wildcards: outputDir(f"tmp/{locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item()}/removed_rare_variants.psam"),
    output:
        linkage_report=directory(outputDir("linkage_disequilibrium/{cluster}/{location}/"))
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --loop-cats {wildcards.cluster} --from-bp {params.fromBP} --to-bp {params.toBP} --chr {params.chr} --r2-unphased yes-really zs cols=chrom,pos,id,ref,alt,provref,maj --ld-window 100000 --out {params.output} >{log} 2>&1
        """

rule pca:
    log: outputDir("pca/{location}/pca.log")
    benchmark: outputDir("pca/{location}/pca.benchmark")
    wildcard_constraints: # TODO: Make this configurable
        contig=r"[0-9]{1,2}"
    params:
        fromBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "ld_start"].item(),
        toBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "ld_stop"].item(),
        chr=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item(),
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards: outputDir(f"pca/{wildcards.location}/pca")
    wildcard_constraints:
        cluster=r"[a-zA-Z0-9\-]+",
        location=r"[a-zA-Z0-9\-]+"
    input:
        pgen=lambda wildcards: outputDir(f"tmp/{locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item()}/removed_rare_variants.pgen"),
        pvar=lambda wildcards: outputDir(f"tmp/{locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item()}/removed_rare_variants.pvar.zst"),
        psam=lambda wildcards: outputDir(f"tmp/{locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item()}/removed_rare_variants.psam"),
    output:
        eigenvectors=outputDir("pca/{location}/pca.eigenvec"),
        eigenvectorsPerAllele=outputDir("pca/{location}/pca.eigenvec.allele"),
        eigenvalues=outputDir("pca/{location}/pca.eigenval"),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --from-bp {params.fromBP} --to-bp {params.toBP} --chr {params.chr} --pca allele-wts --maf 0.1 --out {params.output} >{log} 2>&1
        """


rule extract_provided_coordinates:
    log: outputDir("tmp/{location}/extracted_provided_coordinates.log"),
    benchmark: outputDir("tmp/{location}/extracted_provided_coordinates.benchmark")
    wildcard_constraints:
        location=r"[a-zA-Z0-9\-]+"
    params:
        fromBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "start"].item(),
        toBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "stop"].item(),
        chr=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item(),
        input=lambda wildcards, input: input["pgen"].replace('.pgen', ""),
        output=lambda wildcards, output: output["pgen"].replace(".pgen", ""),
    input:
        pgen=lambda wildcards: outputDir(f"tmp/{locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item()}/removed_related_samples.pgen"),
        pvar=lambda wildcards: outputDir(f"tmp/{locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item()}/removed_related_samples.pvar.zst"),
        psam=lambda wildcards: outputDir(f"tmp/{locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item()}/removed_related_samples.psam"),
    output:
        pgen=outputDir("tmp/{location}/extracted_provided_coordinates.pgen"),
        pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        psam=outputDir("tmp/{location}/extracted_provided_coordinates.psam"),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --from-bp {params.fromBP} --to-bp {params.toBP} --chr {params.chr} --make-pgen vzs --out {params.output} >{log} 2>&1
        """

# Redundant implementation.
# rule report_variant_frequency_by_region:
#     log: outputDir("tmp/{location}/extracted_provided_coordinates.log"),
#     benchmark: outputDir("tmp/{location}/extracted_provided_coordinates.benchmark")
#     params:
#         input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
#         out=lambda wildcards,output: output["allele_count"].replace(".acount", ""),
#     input:
#         pgen=outputDir("tmp/{location}/extracted_provided_coordinates.pgen"),
#         pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
#         psam=outputDir("tmp/{location}/extracted_provided_coordinates.psam"),
#     output:
#         allele_count=outputDir("tmp/{location}/All.acount"),
#     threads: workflow.cores * 0.25
#     shell:
#         """
#         plink2 --threads {threads} --pfile {params.input} vzs --freq counts --out {params.out} >{log} 2>&1
#         """


checkpoint report_count_partitioned_per_cluster:
    log: outputDir("tmp/{cluster}/{location}/reported_frequency_per_cluster/allele_count.log"),
    benchmark: outputDir("tmp/{cluster}/{location}/reported_frequency_per_cluster/allele_count.benchmark")
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards: outputDir(f"tmp/{wildcards.cluster}/{wildcards.location}/freqByCluster/allele_count"),
    input:
        pgen=outputDir("tmp/{location}/extracted_provided_coordinates.pgen"),
        pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        psam=outputDir("tmp/{location}/extracted_provided_coordinates.psam")
    output:
        files=directory(outputDir("tmp/{cluster}/{location}/reported_frequency_per_cluster/"))
    wildcard_constraints:
        location=r"[a-zA-Z0-9\-]+",
        cluster=r"[a-zA-Z0-9\-]+",
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --loop-cats {wildcards.cluster} --freq counts cols=chrom,pos,ref,alt,reffreq,altfreq,nobs --out {params.output} >{log} 2>&1
        """


# Redundant code.
# checkpoint reportHardyWeinbergAllPerLocation:
#     log: outputDir("tmp/{cluster}/{location}/hardy_weinberg.log"),
#     benchmark: outputDir("tmp/{cluster}/{location}/hardy_weinberg.benchmark")
#     params:
#         input= lambda wildcards, input: input["pgen"].replace(".pgen", ""), 
#         output=lambda wildcards,output: outputDir(f"tmp/reportHardyWeinbergAllPerLocation/{wildcards.cluster}/{wildcards.location}/hardy_weinberg"),
#     wildcard_constraints:
#         cluster=r"[a-zA-Z0-9\-]+",
#         location=r"[a-zA-Z0-9\-]+"
#     input:
#         pgen=outputDir("tmp/{location}/extracted_provided_coordinates.pgen"),
#         pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
#         psam=outputDir("tmp/{location}/extracted_provided_coordinates.psam"),
#     output:
#         directory(outputDir("tmp/reportHardyWeinbergAllPerLocation/{cluster}/{location}/")),
#     threads: workflow.cores * 0.25
#     shell:
#         """
#         plink2 --threads {threads} --pfile {params.input} vzs --hardy midp cols=chrom,pos,ref,alt,gcounts,hetfreq,p --out {params.output} >{log} 2>&1
#         """


# Redundant code.
# checkpoint report_sex_linked_hardy_weinberg_per_cluster:
#     log: outputDir("tmp/reportHardyWeinbergAllPerLocationOnChrX/{cluster}/{location}/hardy_weinberg.log"),
#     benchmark: outputDir("tmp/reportHardyWeinbergAllPerLocationOnChrX/{cluster}/{location}/hardy_weinberg.benchmark")
#     params:
#         input= lambda wildcards, input: input["pgen"].replace(".pgen", ""), 
#         output=lambda wildcards,output: outputDir(f"tmp/reportHardyWeinbergAllPerLocationOnChrX/{wildcards.cluster}/{wildcards.location}/hardy_weinberg"),
#     input:
#         pgen=outputDir("tmp/{location}/extracted_provided_coordinates.pgen"),
#         pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
#         psam=outputDir("tmp/{location}/extracted_provided_coordinates.psam"),
#     output:
#         outputDir("tmp/reportHardyWeinbergAllPerLocationOnChrX/{cluster}/{location}/hardy_weinberg.hardy.x"),
#     threads: workflow.cores * 0.25
#     shell:
#         """
#         plink2 --threads {threads} --pfile {params.input} vzs --hardy midp cols=chrom,pos,ref,alt,gcounts,hetfreq,p --out {params.output} >{log} 2>&1
#         """


checkpoint report_hardy_weinberg_per_cluster:
    log: outputDir("tmp/{cluster}/{location}/hardy_weinberg_per_cluster/hardy_weinberg.log"),
    benchmark: outputDir("tmp/{cluster}/{location}/hardy_weinberg_per_cluster/hardy_weinberg.benchmark")
    params:
        input= lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards: outputDir(f"tmp/{wildcards.cluster}/{wildcards.location}/hardy_weinberg_per_cluster/hardy_weinberg")
    wildcard_constraints:
        cluster=r"[a-zA-Z0-9\-]+",
        location=r"[a-zA-Z0-9\-]+"
    input:
        pgen=outputDir("tmp/{location}/extracted_provided_coordinates.pgen"),
        pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        psam=outputDir("tmp/{location}/extracted_provided_coordinates.psam"),
    output:
        directory=directory(outputDir("tmp/{cluster}/{location}/hardy_weinberg_per_cluster/")),
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --loop-cats {wildcards.cluster} --hardy midp cols=chrom,pos,ref,alt,gcounts,hetfreq,p --out {params.output} >{log} 2>&1
        """


rule reportHardyWeinbergPartitionedPerClusterPerLocationOnChrX:
    log: outputDir("tmp/{cluster}/{location}/hardy_weinberg_x.log"),
    benchmark: outputDir("tmp/{cluster}/{location}/hardy_weinberg_x.benchmark")
    params:
        out=lambda wildcards: outputDir(f"tmp/{wildcards.cluster}/{wildcards.location}/hardy_weinberg_x")
    wildcard_constraints:
        cluster=r"[a-zA-Z0-9\-]+",
        location=r"[a-zA-Z0-9\-]+",
    input:
        vcf=outputDir("tmp/{location}/extracted_provided_coordinates.vcf.gz")
    output:
        directory(outputDir("tmp/{cluster}/{location}/"))
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --vcf {input.vcf} --loop-cats {wildcards.cluster} --hardy midp cols=chrom,pos,ref,alt,gcounts,hetfreq,p --out {params.out} >{log} 2>&1
        """

# Move to Population Structure Workflow
checkpoint report_fixation_index_per_cluster:
    log: outputDir("fixation_index/{cluster}/{location}/fixation_index_per_cluster.log")
    benchmark: outputDir("fixation_index/{cluster}/{location}/fixation_index_per_cluster.benchmark")
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards: outputDir(f"fixation_index/{wildcards.cluster}/{wildcards.location}/fixation_index_per_cluster")
    wildcard_constraints:
        cluster=r"[a-zA-Z0-9\-]+",
        location=r"[a-zA-Z0-9\-]+"
    input:
        pgen=outputDir("tmp/{location}/extracted_provided_coordinates.pgen"),
        pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        psam=outputDir("tmp/{location}/extracted_provided_coordinates.psam"),
    output:
        fixation_report=directory(outputDir("fixation_index/{cluster}/{location}/"))
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --fst {wildcards.cluster} report-variants zs --out {params.output} >{log} 2>&1
        """


checkpoint report_missingness_per_cluster:
    log: outputDir("tmp/{cluster}/{location}/missingness_per_cluster/missingness.log")
    benchmark: outputDir("tmp/{cluster}/{location}/missingness_per_cluster/missingness.benchmark")
    params:
        input=lambda wildcards, input: input["pgen"].replace(".pgen", ""),
        output=lambda wildcards: outputDir(f"tmp/{wildcards.cluster}/{wildcards.location}/missingness_per_cluster/missingness")
    wildcard_constraints:
        cluster=r"[a-zA-Z0-9\-]+",
        location=r"[a-zA-Z0-9\-]+"
    input:
        pgen=outputDir("tmp/{location}/extracted_provided_coordinates.pgen"),
        pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        psam=outputDir("tmp/{location}/extracted_provided_coordinates.psam"),
    output:
        missingness_reports=directory(outputDir("tmp/{cluster}/{location}/missingness_per_cluster/"))
    threads: workflow.cores * 0.25
    shell:
        """
        plink2 --threads {threads} --pfile {params.input} vzs --loop-cats {wildcards.cluster} --missing zs vcols=chrom,pos,ref,alt,provref,nmiss,nobs,fmiss --out {params.output} >{log} 2>&1
        """


rule collect_variant_count:
    log: outputDir("tmp/{cluster}/{location}/variant_count.log")
    benchmark: outputDir("tmp/{cluster}/{location}/variant_count.benchmark")
    wildcard_constraints:
        location=r"[a-zA-Z0-9\-]+",
        cluster=r"[a-zA-Z0-9\-]+",
    threads: workflow.cores * 0.25
    input:
        pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        allele_counts=collect_report_count_partitioned_per_cluster,
    output:
        allele_counts=outputDir("tmp/{cluster}/{location}/variant_count.csv")
    script:
        join("scripts", "collectCountPartitionedPerClusterPerLocation.py")


rule collect_variant_frequency:
    log: outputDir("tmp/{cluster}/{location}/variant_frequency.log")
    benchmark: outputDir("tmp/{cluster}/{location}/variant_frequency.benchmark")
    wildcard_constraints:
        location=r"[a-zA-Z0-9\-]+",
        cluster=r"[a-zA-Z0-9\-]+",
    threads: workflow.cores * 0.25
    input:
        pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        psam=outputDir("tmp/{location}/extracted_provided_coordinates.psam"),
        allele_counts=outputDir("tmp/{cluster}/{location}/variant_count.csv"),
    output:
        file=outputDir("tmp/{cluster}/{location}/variant_frequency.csv")
    script:
        join("scripts", "collectFreqPartitionedPerClusterPerLocation.py")


rule report_fishers_exact_with_corrections:
    log: outputDir("tmp/{cluster}/{location}/fishers_exact.log")
    benchmark: outputDir("tmp/{cluster}/{location}/fishers_exact.benchmark")
    params:
        reference_population=lambda wildcards: config["fishers-test"][wildcards.cluster],
    wildcard_constraints:
        location=r"[a-zA-Z0-9\-]+",
        cluster=r"[a-zA-Z0-9\-]+",
    threads: workflow.cores * 0.25
    input:
        psam=outputDir("tmp/{location}/extracted_provided_coordinates.psam"),
        allele_counts=outputDir("tmp/{cluster}/{location}/variant_count.csv"),
    output:
        outputDir("tmp/{cluster}/{location}/fishers_exact_with_corrections.csv")
    script:
        join("scripts", "calculateFishersExactTestWithCorrection.py")


rule collect_autosomal_hardy_weinberg:
    log: outputDir("tmp/{cluster}/{location}/autosomal_hardy_weinberg.log")
    benchmark: outputDir("tmp/{cluster}/{location}/autosomal_hardy_weinberg.benchmark")
    wildcard_constraints:
        location=r"[a-zA-Z0-9\-]+",
        cluster=r"[a-zA-Z0-9\-]+",
    threads: workflow.cores * 0.25
    input:
        pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        hardy_weinberg=collect_autosomal_hardy_weinberg_per_cluster,
    output:
        outputDir("tmp/{cluster}/{location}/autosomal_hardy_weinberg.csv")
    script:
        join("scripts", "collectAutosomalHardyWeinbergPartitionedPerClusterPerLocation.py")


rule collect_variant_missingness:
    log: outputDir("tmp/{cluster}/{location}/missingnessPartitionedPerCluster.log")
    benchmark: outputDir("tmp/{cluster}/{location}/missingnessPartitionedPerCluster.benchmark")
    wildcard_constraints:
        location=r"[a-zA-Z0-9\-]+",
        cluster=r"[a-zA-Z0-9\-]+",
    threads: workflow.cores * 0.25
    input:
        pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        missingness_reports=collect_report_missingness_per_cluster,
    output:
        variant_missingness_report=outputDir("tmp/{cluster}/{location}/missingness.csv")
    script:
        join("scripts", "collectVariantMissingnessPerClusterPerLocation.py")


rule query_variant_effect_predictions:
    log: outputDir("tmp/{cluster}/{location}/variant_effect_predictions.log"),
    benchmark: outputDir("tmp/{cluster}/{location}/variant_effect_predictions.benchmark")
    wildcard_constraints:
        location=r"[a-zA-Z0-9\-]+",
        cluster=r"[a-zA-Z0-9\-]+",
    threads: workflow.cores * 0.25
    params:
        strand=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "strand"]
    input:
        pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
    output:
        outputDir("tmp/{cluster}/{location}/variant_effect_predictions.csv")
    script:
        join("scripts", "query_variant_effects.py")


rule compile_variant_effect_predictions:
    log: outputDir("tmp/{cluster}/{location}/cleaned_variant_effect_predictions.log"),
    benchmark: outputDir("tmp/{cluster}/{location}/cleaned_variant_effect_predictions.benchmark")
    wildcard_constraints:
        location=r"[a-zA-Z0-9\-]+",
        cluster=r"[a-zA-Z0-9\-]+",
    threads: workflow.cores * 0.25
    input:
        vep_results=outputDir("tmp/{cluster}/{location}/variant_effect_predictions.csv")
    output:
        cleaned_vep_results=outputDir("tmp/{cluster}/{location}/cleaned_variant_effect_predictions.csv")
    script:
        join("scripts", "compile_variant_effects.py")


rule consolidate_reports:
    log: outputDir("consolidated_reports/{cluster}_{location}.log")
    benchmark: outputDir("consolidated_reports/{cluster}_{location}.benchmark")
    wildcard_constraints:
        location=r"[a-zA-Z0-9\-]+",
        cluster=r"[a-zA-Z0-9\-]+",
    threads: workflow.cores * 0.25
    input:
        pvar=outputDir("tmp/{location}/extracted_provided_coordinates.pvar.zst"),
        analyses=collect_reports_to_consolidate,
    output:
        consolidated_data=outputDir("consolidated_reports/{cluster}_{location}.csv")
    script:
        join("scripts", "consolidateReports.py")


rule all:
    default_target: True
    log:
        "logs/ALL/ALL.log",
    input:
        expand(outputDir("consolidated_reports/{cluster}_{location}.csv"), cluster=clusters, location=locations["location_name"]),
        collect_calculate_linkage_disequilibrium_per_cluster,
        collect_report_fixation_index_per_cluster,
        expand(outputDir("pca/{location}/pca.eigenvec"), location=locations["location_name"]),
        expand(outputDir("pca/{location}/pca.eigenvec.allele"), location=locations["location_name"]),
        expand(outputDir("pca/{location}/pca.eigenval"), location=locations["location_name"]),
