from pandas import read_csv
from os.path import join
from snakemake.utils import validate
from snakemake.utils import min_version

__author__ = "Graeme Ford"
__credits__ = [
    "Graeme Ford",
    "Prof. Michael S. Pepper",
    "Prof. Fourie Joubert",
    "Antionette Colic",
    "Fatima Barmania",
    "Sarah Turner",
    "Megan Holborn",
]
__version__ = "1.0.0"
__maintainer__ = "Graeme Ford"
__email__ = "graeme.ford@tuks.co.za"
__status__ = "Development"

# Enforce version check
min_version("7.24.2")


# DEFINE CONFIG FILE FOR SNAKEMAKE:
configfile: join("config", "config.json")


# SET REPORT TEMPLATE
report: "report/template.rst"


# validate(config, join("..", "config", ".schema", "config.schema.json"))


# pepfile: join("config", "pep.yaml")


# pepschema: join("..", "config", ".schema", "pep.schema.yaml")

locations = read_csv(join("input", "locations.csv"), header=0)
samples = read_csv(join("input", "samples.csv"), header=0)
datasets = read_csv(join("input", "datasets.csv"), header=0)
transcripts = read_csv(join("input", "transcripts.csv"), header=0)


# DEFINE CONTEXT-VARIABLES:
finalExtensions = ["acount", "hardy", "smiss", "vmiss"]  # "prune.in", "prune.out",
clusters = set([cluster for cluster in samples.keys() if cluster not in ["sample_name", "dataset"]])
bExtensions = ["bed", "bim", "fam"]
tExtensions = ["map", "ped"]


include: "rules/common.py"

# DEFINE CONTAINERIZED ENVIRONMENT:
container: "docker://graemeford/pipeline-os"

module VCFValidation:
    snakefile: github("Tuks-ICMM/Vcf-Validation-Workflow", path="workflow/Snakefile", branch="user/GraemeFord")
    config: config

use rule * from VCFValidation as VCF_Validation_*
ruleorder: VCF_Validation_tabix > tabix

module PopulationStructure:
    snakefile: github("Tuks-ICMM/Population-Structure-Workflow", path="workflow/Snakefile", branch="main")
    config: config

use rule * from PopulationStructure as PopulationStructure_*

use rule Admixture_v1p3 from PopulationStructure as PopulationStructure_Admixture_v1p3 with:
    input:
        "results/FILTER/ALL_FILTERED.vcf.gz"
use rule Plink_PCA from PopulationStructure as PopulationStructure_Plink_PCA with:
    input:
        "results/FILTER/ALL_FILTERED.vcf.gz"
use rule DAPC from PopulationStructure as PopulationStructure_DAPC with:
    input:
        "results/FILTER/ALL_FILTERED.vcf.gz"

# BEGIN DEFINING RULES:
rule all:
    """
    Catch-all rule to trigger auto-run of all processes. This process will be fired automatically in absence of explicit process name given by cli-argument.
    """
    default_target: True
    log:
        "logs/ALL/ALL.log",
    input:
        VCFValidation.rules.all.input,
        PopulationStructure.rules.all.input,
        [
            expand(
                [
                    "results/FINAL/%s/ALL_{location}.%s.{extension}"
                    % (cluster, population)
                    for population in list(samples[cluster].unique())
                ],
                extension=finalExtensions,
                location=locations["location_name"],
            )
            for cluster in clusters
        ]


def VcfValidationAdapter(_):
    """
    An adapter to generate the correct input list from `VCF Validation Pipeline`. This is required as liftover is optional, making the output files variable.
    """
    for dataset in datasets["reference_genome"]:
        if dataset != "GRCh38":
            return {
                'vcf': expand("results/PREP/{dataset}/liftover.vcf.gz", dataset=datasets["dataset_name"].tolist()),
                "tabix": expand("results/PREP/{dataset}/liftover.vcf.gz.tbi", dataset=datasets["dataset_name"].tolist())
                }
        else:
            return {
                'vcf': expand("results/PREP/{dataset}/annotate.vcf.gz", dataset=datasets["dataset_name"].tolist()),
                "tabix": expand("results/PREP/{dataset}/annotate.vcf.gz.tbi", dataset=datasets["dataset_name"].tolist())
                }

rule merge:
    """
    """
    group: "COLLATE"
    input:
        unpack(VcfValidationAdapter)
    output:
        "results/COLLATE/merge.vcf.gz"
    log:
        "_logs/COLLATE/merge.log"
    benchmark:
        "_benchmarks/merge/merge.benchmark"
    resources:
        cpus=search("cores", "COLLATE"),
        nodes=search("nodes", "COLLATE"),
        queue=search("queue", "COLLATE"),
        walltime=search("walltime", "COLLATE"),
    envmodules:
        config["environment"]["envmodules"]["bcftools"]
    shell:
        """
        bcftools merge -O z -o {output} {input.vcf}
        """

rule refFromFasta:
    """
    """
    group: "COLLATE"
    input: 
        lambda wildcards: unpack(VcfValidationAdapter) if datasets.shape[0] <=1 else expand("results/COLLATE/merge.{ext}", ext=["vcf.gz", "vcf.gz.tbi"])
    output: 
        "results/COLLATE/refFromFasta.vcf.gz"
    log:
        "_logs/COLLATE/refFromFasta.log"
    benchmark:
        "_benchmarks/COLLATE/refFromFasta.benchmark"
    resources:
        cpus=search("cores", "COLLATE"),
        nodes=search("nodes", "COLLATE"),
        queue=search("queue", "COLLATE"),
        walltime=search("walltime", "COLLATE"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"]
    params:
        ref=lambda wildcards: join(
        *next(
            i["file_path"]
            for i in config["reference-genomes"]
        if i["version"] == "GRCh38"
            ),
        )
    shell:
        """
        plink2 --vcf {input[0]} --fa {params.ref} --ref-from-fa force --allow-extra-chr --export vcf-4.2 bgz --out results/COLLATE/refFromFasta
        """

rule chrFilter:
    """
    """
    group: "COLLATE"
    input:
        "results/COLLATE/refFromFasta.vcf.gz",
        "results/COLLATE/refFromFasta.vcf.gz.tbi"
    output:
        "results/COLLATE/chrFilter.vcf.gz"
    log:
        "_logs/COLLATE/chrFilter.log"
    benchmark:
        "_benchmarks/COLLATE/chrFilter.benchmark"
    resources:
        cpus=search("cores", "COLLATE"),
        nodes=search("nodes", "COLLATE"),
        queue=search("queue", "COLLATE"),
        walltime=search("walltime", "COLLATE"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"]
    shell:
        """
        plink2 --vcf {input[0]} --allow-extra-chr --output-chr chr26 --chr 1-22 --export vcf-4.2 bgz --out results/COLLATE/chrFilter
        """

# group: VALIDATE
# conditional: true
rule sampleSubset:
    """
    Subset samples according to user defined list and remove variants that do not pass QC.
    """
    group: "VALIDATE"
    input:
        vcf="results/COLLATE/chrFilter.vcf.gz",
        samples="input/samples.csv"
    output:
        "results/VALIDATE/sampleSubset.vcf.gz"
    log:
        "_logs/VALIDATE/sampleSubset.log"
    benchmark:
        "_benchmarks/VALIDATE/sampleSubset.benchmark"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    params:
        samples=lambda wildcards, input: ",".join(read_csv(join("input", "samples.csv"), header=0)["sample_name"].tolist())
    shell:
        """
        bcftools view -s {params.samples} -O z -o {output} {input.vcf}  2>{log}
        """


rule FILTER:
    """
    Filter out individuals and variants (Safety Check).
    """
    log:
        "logs/FILTER/FILTER.log",
    input:
        "results/VALIDATE/sampleSubset.vcf.gz"
    output:
        "results/FILTER/ALL_FILTERED.vcf.gz",
    params:
        ref=lambda wildcards: join(
        *next(
            i["file_path"]
            for i in config["reference-genomes"]
        if i["version"] == "GRCh38"
            ),
        ),    
    resources:
        cpus=search("cores", "FILTER"),
        nodes=search("nodes", "FILTER"),
        queue=search("queue", "FILTER"),
        walltime=search("walltime", "FILTER"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"],
        config["environment"]["envmodules"]["bcftools"],
        config["environment"]["envmodules"]["plink-1.9"],
    shell:
        """
        # TODO: Reorganise this whole section into sub-workflow
        # Filter out variants with >= 100% missingness
        echo -e "\n--- LOG SECTION START | Plink-2 'Filter 100% Missingness variants' ---" 1>&2
        plink2 --chr 1-22 --allow-extra-chr --vcf {input} --geno 1.0 --output-chr chr26 --export vcf-4.2 bgz --out results/FILTER/ALL_FILTERED1
        echo -e "--- LOG SECTION END | Plink-2 'Filter 100% Missingness variants' ---\n" 1>&2

        # Filter out samples with >= 100% missingness
        echo -e "\n--- LOG SECTION START | Plink-2 'Filter 100% Missingness samples' ---" 1>&2
        plink2 --chr 1-22 --allow-extra-chr --vcf results/FILTER/ALL_FILTERED1.vcf.gz --mind 1.0 --output-chr chr26 --export vcf-4.2 bgz --out results/FILTER/ALL_FILTERED2
        echo -e "--- LOG SECTION END | Plink-2 'Filter 100% Missingness samples' ---\n" 1>&2
                
        # TODO
        # Remove variants with reference alleles not matching that of reference genome
        # "module load bcftools-1.7; bcftools norm --check-ref x results/FILTER/ALL_FILTERED1.vcf.gz --fasta-ref {params.ref} -o results/FILTER/ALL_FILTERED2.vcf.gz -O z"
        
        # Filter out first- and second-degree related individuals and duplicate samples
        ## Create a linkage disequilibrium-pruned subset of the data and perform IBD calculations on this data subset. 
        echo -e "\n--- LOG SECTION START | Plink-2 'Filter -first and second-degree related individuals' ---" 1>&2
        plink2 --chr 1-22 --set-all-var-ids @_#\$1,\$2 --new-id-max-allele-len 500 --rm-dup exclude-mismatch --vcf results/FILTER/ALL_FILTERED2.vcf.gz --indep-pairwise 50 5 0.5 --bad-ld --export vcf-4.2 bgz  --out results/FILTER/ALL_FILTERED_LD
        plink2 --allow-extra-chr --vcf results/FILTER/ALL_FILTERED_LD.vcf.gz --extract results/FILTER/ALL_FILTERED_LD.prune.in --export vcf-4.2 bgz --out results/FILTER/ALL_FILTERED_LD_pruned
        plink --allow-extra-chr --vcf results/FILTER/ALL_FILTERED_LD_pruned.vcf.gz --genome --min 0.2 --recode vcf-iid bgz --out results/FILTER/ALL_FILTERED_LD_unrelated
        echo -e "--- LOG SECTION END | Plink-2 'Filter -first and second-degree related individuals' ---\n" 1>&2
        
        ## Extract the sample IDs of all unrelated individuals
        echo -e "\n--- LOG SECTION START | BcfTools 'query' ---" 1>&2
        bcftools query -l results/FILTER/ALL_FILTERED_LD_unrelated.vcf.gz > results/FILTER/unrelated_samples.txt
        echo -e "--- LOG SECTION END | BcfTools 'query' ---\n" 1>&2
        
        # Subset the data by these unrelated individuals
        echo -e "\n--- LOG SECTION START | BcfTools 'view' ---" 1>&2
        bcftools view results/FILTER/ALL_FILTERED2.vcf.gz -S results/FILTER/unrelated_samples.txt -o {output} -O z
        echo -e "--- LOG SECTION END | BcfTools 'view' ---\n" 1>&2
        """
    


rule ADMIXTURE:
    """
    Perform Admixture analysis on the large psudo-dataset (Requires 100 000 minimum variants to distinguish sub-populations and 10 000 to distinguish super-populations.)
    """
    log:
        "logs/ADMIXTURE/{cluster}.{estimation}.log",
    input:
        "results/FILTER/ALL_FILTERED.vcf.gz",
    output:
        expand(
            "results/ADMIXTURE/{{cluster}}.{{estimation}}.{ext}",
            ext=["bed", "bim", "fam"],
        ),
        expand(
            "results/FINAL/Admixture/{{cluster}}.{{estimation}}.{mode}",
            mode=["P", "Q"],
        ),
    resources:
        cpus=search("cores", "ADMIXTURE"),
        nodes=search("nodes", "ADMIXTURE"),
        queue=search("queue", "ADMIXTURE"),
        walltime=search("walltime", "ADMIXTURE"),
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
        config["environment"]["envmodules"]["plink-2"],
        config["environment"]["envmodules"]["admixture-1.3"],
    shell:
        """
        # Filter to only biallelic SNPs
        echo -e "\n--- LOG SECTION START | BcfTools 'view' ---" 1>&2
        bcftools view -O z -o results/ADMIXTURE/BIALLELIC.vcf.gz -m2 -M2 -v snps {input}
        echo -e "--- LOG SECTION END | BcfTools 'view' ---\n" 1>&2

        echo -e "\n--- LOG SECTION START | Plink-2 'thin to 200 000 variants' ---" 1>&2
        plink2 --allow-extra-chr --vcf results/ADMIXTURE/BIALLELIC.vcf.gz --thin-count 200000 --set-missing-var-ids @_# --make-bed --out results/ADMIXTURE/{wildcards.cluster}.{wildcards.estimation}
        echo -e "--- LOG SECTION END | Plink-2 'thin to 200 000 variants' ---\n" 1>&2
        
        echo -e "\n--- LOG SECTION START | Admixture-1.3.0 ---" 1>&2
        admixture results/ADMIXTURE/{wildcards.cluster}.{wildcards.estimation}.bed {wildcards.estimation}
        mv {wildcards.cluster}.{wildcards.estimation}.{wildcards.estimation}.P results/FINAL/Admixture/{wildcards.cluster}.{wildcards.estimation}.P
        mv {wildcards.cluster}.{wildcards.estimation}.{wildcards.estimation}.Q results/FINAL/Admixture/{wildcards.cluster}.{wildcards.estimation}.Q
        echo -e "--- LOG SECTION END | Admixture-1.3.0 ---\n" 1>&2
        """


rule TRIM_AND_NAME:
    """
    Trim the whole-genome psudo-datasets down to several regions of interest for Variant analysis and Variant effect prediction.
    """
    log:
        "logs/TRIM/{location}.log",
    input:
        "results/FILTER/ALL_FILTERED.vcf.gz",
    output:
        "results/TRIM/ALL_{location}.vcf.gz",
    params:
        fromBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "start"].item(),
        toBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "stop"].item(),
        chr=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item(),
    resources:
        cpus=search("cores", "TRIM_AND_NAME"),
        nodes=search("nodes", "TRIM_AND_NAME"),
        queue=search("queue", "TRIM_AND_NAME"),
        walltime=search("walltime", "TRIM_AND_NAME"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"],
    shell:
        """
        echo -e "\n--- LOG SECTION START | Plink-2 'trim' ---" 1>&2
        plink2 --allow-extra-chr --vcf {input} --from-bp {params.fromBP} --to-bp {params.toBP} --chr {params.chr} --output-chr chr26 --export vcf-4.2 bgz --out results/TRIM/ALL_{wildcards.location}
        echo -e "--- LOG SECTION END | Plink-2 'trim' ---\n" 1>&2
        """
    


rule TRANSPILE_CLUSTERS:
    """
    Convert Cluster information given in the config file into PLINK-2.0 suitable format.
    """
    log:
        "logs/TRANSPILE/{cluster}.log",
    output:
        "results/REFERENCE/cluster_{cluster}.txt",
    resources:
        cpus=search("cores", "TRANSPILE_CLUSTERS"),
        nodes=search("nodes", "TRANSPILE_CLUSTERS"),
        queue=search("queue", "TRANSPILE_CLUSTERS"),
        walltime=search("walltime", "TRANSPILE_CLUSTERS"),
    conda:
        join("envs", "snakemake.yml")
    envmodules:
        config["environment"]["envmodules"]["python-3"]
    script:
        join("scripts", "01-TRANSPILE_CLUSTERS.py")


rule PLINK:
    """
    Perform Frequency analysis on super populations.
    """
    log:
        "logs/PLINK/{location}.log",
    input:
        ["results/REFERENCE/cluster_%s.txt" % (cluster) for cluster in ["SUPER", "SUB"]],
        vcf="results/TRIM/ALL_{location}.vcf.gz",
    output:
        [
            expand(
                [
                    "results/FINAL/%s/ALL_{{location}}.%s.{extension}"
                    % (cluster, population)
                    for population in list(samples[cluster].unique())
                ],
                extension=finalExtensions,
            )
            for cluster in clusters
        ],
    params:
        prefix="ALL_{location}",
        subsets_list=lambda wildcards: ' '.join(clusters)
    resources:
        cpus=search("cores", "PLINK"),
        nodes=search("nodes", "PLINK"),
        queue=search("queue", "PLINK"),
        walltime=search("walltime", "PLINK"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"],
    shell:
        """
        for CLUSTER in {params.subsets_list}
        do
            echo -e "\n--- LOG SECTION START | Plink-2 'All freq' ---" 1>&2
            plink2 --allow-extra-chr --vcf {input.vcf} --freq counts --export vcf-4.2 bgz --out results/FINAL/$CLUSTER/{params.prefix}
            echo -e "--- LOG SECTION END | Plink-2 'All freq' ---\n" 1>&2
            
            echo -e "\n--- LOG SECTION START | Plink-2 'Subset freq' ---" 1>&2
            plink2 --allow-extra-chr --vcf {input.vcf} --pheno iid-only results/REFERENCE/cluster_$CLUSTER.txt --loop-cats $CLUSTER --freq counts --missing --hardy midp --out results/FINAL/$CLUSTER/{params.prefix}
            echo -e "--- LOG SECTION END | Plink-2 'Subset freq' ---\n" 1>&2
        done
        """
    
# group: VALIDATE
rule tabix:
    """
    Generate tabix-index.
    """
    input:
        "results/{operation}/{output}.vcf.gz"
    output:
        "results/{operation}/{output}.vcf.gz.tbi"
    log:
        "_logs/{operation}/{output}.log"
    benchmark:
        "_benchmarks/{operation}/{output}.benchmark"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    shell:
        """
        tabix -p vcf {input}
        """