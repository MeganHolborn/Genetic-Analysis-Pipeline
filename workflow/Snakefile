from pandas import read_csv
from os.path import join
from snakemake.utils import validate
from snakemake.utils import min_version

__author__ = "Graeme Ford"
__credits__ = [
    "Graeme Ford",
    "Prof. Michael S. Pepper",
    "Prof. Fourie Joubert",
    "Antionette Colic",
    "Fatima Barmania",
    "Sarah Turner",
    "Megan Holborn",
]
__version__ = "1.0.0"
__maintainer__ = "Graeme Ford"
__email__ = "graeme.ford@tuks.co.za"
__status__ = "Development"

# Enforce version check
min_version("7.24.2")


# DEFINE CONFIG FILE FOR SNAKEMAKE:
configfile: join("config", "config.json")

# LD_LIBRARY_PATH is required to inform Python where OpenSSL library files are located. This is included otherwise none of the rules queued on the system will have this variable set.
envvars:
    "LD_LIBRARY_PATH"

# SET REPORT TEMPLATE
report: "report/template.rst"


# validate(config, join("..", "config", ".schema", "config.schema.json"))


# pepfile: join("config", "pep.yaml")


# pepschema: join("..", "config", ".schema", "pep.schema.yaml")

locations = read_csv(join("input", "locations.csv"), header=0)
samples = read_csv(join("input", "samples.csv"), header=0)
datasets = read_csv(join("input", "datasets.csv"), header=0)
transcripts = read_csv(join("input", "transcripts.csv"), header=0)


# DEFINE CONTEXT-VARIABLES:
finalExtensions = ["acount", "hardy", "smiss", "vmiss"]  # "prune.in", "prune.out",
clusters = set([cluster for cluster in samples.keys() if cluster not in ["sample_name", "dataset"]])
bExtensions = ["bed", "bim", "fam"]
tExtensions = ["map", "ped"]


include: "rules/common.py"

# DEFINE CONTAINERIZED ENVIRONMENT:
container: "docker://graemeford/pipeline-os"

module VCFValidation:
    snakefile: github("Tuks-ICMM/Vcf-Validation-Workflow", path="workflow/Snakefile", branch="main")
    config: config

use rule * from VCFValidation as VCF_Validation_*

# TODO: Document this behaviour for technical users
use rule stats from VCFValidation as VCF_Validation_stats with:
    input:
        final = "results/PREP/{dataset_name}/{operation}.vcf.gz",
        final_tbi = "results/PREP/{dataset_name}/{operation}.vcf.gz.tbi",
        initial = lambda wildcards: getattr(rules, str(wildcards.operation)).input.vcf,
        initial_tbi = lambda wildcards: getattr(rules, str(wildcards.operation)).input.tbi

# TODO: Document this behaviour for technical users
ruleorder: VCF_Validation_tabix > tabix

module PopulationStructure:
    snakefile: github("Tuks-ICMM/Population-Structure-Workflow", path="workflow/Snakefile", branch="main")
    config: config

use rule * from PopulationStructure as PopulationStructure_*

# TODO: Document this behaviour for technical users
use rule Admixture_v1p3 from PopulationStructure as PopulationStructure_Admixture_v1p3 with:
    input:
        "results/FILTER/ALL_FILTERED.vcf.gz"
# TODO: Document this behaviour for technical users
use rule Plink_PCA from PopulationStructure as PopulationStructure_Plink_PCA with:
    input:
        "results/FILTER/ALL_FILTERED.vcf.gz"
# TODO: Document this behaviour for technical users
use rule DAPC from PopulationStructure as PopulationStructure_DAPC with:
    input:
        "results/FILTER/ALL_FILTERED.vcf.gz"

# BEGIN DEFINING RULES:
def VcfValidationAdapter(extension: str) -> list:
    """
    An adapter to generate the correct input list from `VCF Validation Pipeline`. This is required as liftover is optional, making the output files variable.
    """
    merge_list = list()
    for reference_genome, groupby_subset in datasets.set_index(["reference_genome", "dataset_name"]).groupby(level=0): # [FOR] all unqiue (dataset_name and reference_genome) column combinations present
        if reference_genome != "GRCh38" and groupby_subset is not None: # [IF] reference genome version
            for dataset_name in groupby_subset.index.get_level_values("dataset_name"): # [FOR] the column in our MultiIndex that contains the dataset_name's in this subset
                # [EACH] add liftover request for the DAG
                merge_list.append(f"results/PREP/{dataset_name}/liftover{extension}",)
    else:
        for dataset_name in groupby_subset.index.get_level_values("dataset_name"): # [FOR] the column in our MultiIndex that contains the dataset_name's in this subset
                # [EACH] add liftover request for the DAG
                merge_list.append(f"results/PREP/{dataset_name}/annotate{extension}",)
    return merge_list

rule mergeDatasets:
    """
    This rule merges multiple datasets into one large psudo-dataset that can be worked on more easily.
    """
    group: "COLLATE"
    log: "_logs/COLLATE/merge.log"
    benchmark: "_benchmarks/merge/merge.benchmark"
    resources:
        cpus=search("cores", "COLLATE"),
        nodes=search("nodes", "COLLATE"),
        queue=search("queue", "COLLATE"),
        walltime=search("walltime", "COLLATE"),
    envmodules:
        config["environment"]["envmodules"]["bcftools"]
    input:
        vcf=lambda _: VcfValidationAdapter(".vcf.gz"),
        vcfIndex=lambda _: VcfValidationAdapter(".vcf.gz.tbi"),
    output:
        "results/COLLATE/merge.vcf.gz"
    shell:
        """
        bcftools merge -O z -o {output} {input.vcf}
        """

rule refFromFasta:
    """
    This rule verifies that the reference alleles in teh provided VCF file match that of the reference genome.
    """
    group: "COLLATE"
    log: "_logs/COLLATE/refFromFasta.log"
    benchmark: "_benchmarks/COLLATE/refFromFasta.benchmark"
    resources:
        cpus=search("cores", "COLLATE"),
        nodes=search("nodes", "COLLATE"),
        queue=search("queue", "COLLATE"),
        walltime=search("walltime", "COLLATE"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"]
    params:
        ref=lambda wildcards: join(
        *next(
            i["file_path"]
            for i in config["reference-genomes"]
        if i["version"] == "GRCh38"
            ),
        )
    input: 
        vcf=lambda wildcards: expand("results/COLLATE/merge.vcf.gz") if datasets.shape[0] >1 else VcfValidationAdapter(".vcf.gz"),
        vcfIndex=lambda wildcards: expand("results/COLLATE/merge.vcf.gz.tbi") if datasets.shape[0] >1 else VcfValidationAdapter(".vcf.gz.tbi")
    output: 
        "results/COLLATE/refFromFasta.vcf.gz"
    shell:
        """
        plink2 --vcf {input.vcf} --fa {params.ref} --ref-from-fa force --allow-extra-chr --export vcf-4.2 bgz --out results/COLLATE/refFromFasta
        """

rule chrFilter:
    """
    This rule removes any unusual chromosomes.
    """
    group: "FILTER"
    log: "_logs/COLLATE/chrFilter.log"
    benchmark: "_benchmarks/COLLATE/chrFilter.benchmark"
    resources:
        cpus=search("cores", "COLLATE"),
        nodes=search("nodes", "COLLATE"),
        queue=search("queue", "COLLATE"),
        walltime=search("walltime", "COLLATE"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"]
    input:
        "results/COLLATE/refFromFasta.vcf.gz",
        "results/COLLATE/refFromFasta.vcf.gz.tbi"
    output:
        "results/COLLATE/chrFilter.vcf.gz"
    shell:
        """
        plink2 --vcf {input[0]} --allow-extra-chr --output-chr chr26 --chr 1-22 --export vcf-4.2 bgz --out results/COLLATE/chrFilter
        """

rule sampleSubset:
    """
    This rule subsets samples according to user defined list and remove variants that do not pass QC.
    """
    group: "FILTER"
    log: "_logs/VALIDATE/sampleSubset.log"
    benchmark: "_benchmarks/VALIDATE/sampleSubset.benchmark"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    params:
        samples=lambda wildcards, input: ",".join(read_csv(join("input", "samples.csv"), header=0)["sample_name"].tolist())
    input:
        vcf="results/COLLATE/chrFilter.vcf.gz",
        samples="input/samples.csv"
    output:
        "results/VALIDATE/sampleSubset.vcf.gz"
    shell:
        """
        bcftools view -s {params.samples} -O z -o {output} {input.vcf}  2>{log}
        """


rule filterDataset:
    """
    Filter out individuals and variants (Safety Check).
    """
    group: "FILTER"
    log: "logs/FILTER/FILTER.log",
    benchmark: "_benchmarks/VALIDATE/filterDataset.benchmark"
    resources:
        cpus=search("cores", "FILTER"),
        nodes=search("nodes", "FILTER"),
        queue=search("queue", "FILTER"),
        walltime=search("walltime", "FILTER"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"],
        config["environment"]["envmodules"]["bcftools"],
        config["environment"]["envmodules"]["plink-1.9"],
    params:
        ref=lambda wildcards: join(
        *next(
            i["file_path"]
            for i in config["reference-genomes"]
        if i["version"] == "GRCh38"
            ),
        ),    
    input:
        "results/VALIDATE/sampleSubset.vcf.gz"
    output:
        "results/FILTER/ALL_FILTERED.vcf.gz",
    shell:
        """
        # TODO: Reorganise this whole section into sub-workflow
        # Filter out variants with >= 100% missingness
        echo -e "\n--- LOG SECTION START | Plink-2 'Filter 100% Missingness variants' ---" 1>&2
        plink2 --chr 1-22 --allow-extra-chr --vcf {input} --geno 1.0 --output-chr chr26 --export vcf-4.2 bgz --out results/FILTER/ALL_FILTERED1
        echo -e "--- LOG SECTION END | Plink-2 'Filter 100% Missingness variants' ---\n" 1>&2

        # Filter out samples with >= 100% missingness
        echo -e "\n--- LOG SECTION START | Plink-2 'Filter 100% Missingness samples' ---" 1>&2
        plink2 --chr 1-22 --allow-extra-chr --vcf results/FILTER/ALL_FILTERED1.vcf.gz --mind 1.0 --output-chr chr26 --export vcf-4.2 bgz --out results/FILTER/ALL_FILTERED2
        echo -e "--- LOG SECTION END | Plink-2 'Filter 100% Missingness samples' ---\n" 1>&2
                
        # TODO
        # Remove variants with reference alleles not matching that of reference genome
        # "module load bcftools-1.7; bcftools norm --check-ref x results/FILTER/ALL_FILTERED1.vcf.gz --fasta-ref {params.ref} -o results/FILTER/ALL_FILTERED2.vcf.gz -O z"
        
        # Filter out first- and second-degree related individuals and duplicate samples
        ## Create a linkage disequilibrium-pruned subset of the data and perform IBD calculations on this data subset. 
        echo -e "\n--- LOG SECTION START | Plink-2 'Filter -first and second-degree related individuals' ---" 1>&2
        plink2 --chr 1-22 --set-all-var-ids @_#\$1,\$2 --new-id-max-allele-len 500 --rm-dup exclude-mismatch --vcf results/FILTER/ALL_FILTERED2.vcf.gz --indep-pairwise 50 5 0.5 --bad-ld --export vcf-4.2 bgz  --out results/FILTER/ALL_FILTERED_LD
        plink2 --allow-extra-chr --vcf results/FILTER/ALL_FILTERED_LD.vcf.gz --extract results/FILTER/ALL_FILTERED_LD.prune.in --export vcf-4.2 bgz --out results/FILTER/ALL_FILTERED_LD_pruned
        plink --allow-extra-chr --vcf results/FILTER/ALL_FILTERED_LD_pruned.vcf.gz --genome --min 0.2 --recode vcf-iid bgz --out results/FILTER/ALL_FILTERED_LD_unrelated
        echo -e "--- LOG SECTION END | Plink-2 'Filter -first and second-degree related individuals' ---\n" 1>&2
        
        ## Extract the sample IDs of all unrelated individuals
        echo -e "\n--- LOG SECTION START | BcfTools 'query' ---" 1>&2
        bcftools query -l results/FILTER/ALL_FILTERED_LD_unrelated.vcf.gz > results/FILTER/unrelated_samples.txt
        echo -e "--- LOG SECTION END | BcfTools 'query' ---\n" 1>&2
        
        # Subset the data by these unrelated individuals
        echo -e "\n--- LOG SECTION START | BcfTools 'view' ---" 1>&2
        bcftools view results/FILTER/ALL_FILTERED2.vcf.gz -S results/FILTER/unrelated_samples.txt -o {output} -O z
        echo -e "--- LOG SECTION END | BcfTools 'view' ---\n" 1>&2
        """
    

rule filterLocations:
    """
    Trim the whole-genome psudo-datasets down to several regions of interest for Variant analysis and Variant effect prediction.
    """
    group: "FILTER"
    log: "logs/TRIM/{location}.log",
    benchmark: "_benchmarks/filterLocations/{location}.benchmark"
    resources:
        cpus=search("cores", "TRIM_AND_NAME"),
        nodes=search("nodes", "TRIM_AND_NAME"),
        queue=search("queue", "TRIM_AND_NAME"),
        walltime=search("walltime", "TRIM_AND_NAME"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"],
    params:
        fromBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "start"].item(),
        toBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "stop"].item(),
        chr=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item(),
    input:
        "results/FILTER/ALL_FILTERED.vcf.gz",
    output:
        "results/TRIM/ALL_{location}.vcf.gz",
    shell:
        """
        echo -e "\n--- LOG SECTION START | Plink-2 'trim' ---" 1>&2
        plink2 --allow-extra-chr --vcf {input} --from-bp {params.fromBP} --to-bp {params.toBP} --chr {params.chr} --output-chr chr26 --export vcf-4.2 bgz --out results/TRIM/ALL_{wildcards.location}
        echo -e "--- LOG SECTION END | Plink-2 'trim' ---\n" 1>&2
        """
    


rule writeSampleMetadata:
    """
    Convert Cluster information given in the config file into PLINK-2.0 suitable format.
    """
    group: "FILTER"
    log: "logs/TRANSPILE/{cluster}.log",
    benchmark: "_benchmarks/writeSampleMetadata/{cluster}.benchmark"
    resources:
        cpus=search("cores", "TRANSPILE_CLUSTERS"),
        nodes=search("nodes", "TRANSPILE_CLUSTERS"),
        queue=search("queue", "TRANSPILE_CLUSTERS"),
        walltime=search("walltime", "TRANSPILE_CLUSTERS"),
    conda:
        join("envs", "snakemake.yml")
    envmodules:
        config["environment"]["envmodules"]["python-3"]
    output:
        "results/REFERENCE/cluster_{cluster}.txt",
    script:
        join("scripts", "01-TRANSPILE_CLUSTERS.py")


rule reportFreq:
    """
    Perform Frequency analysis on super populations.
    """
    group: "REPORT"
    log: "logs/PLINK/{location}.log",
    benchmark: "_benchmarks/reportFreq/{location}.benchmark"
    resources:
        cpus=search("cores", "PLINK"),
        nodes=search("nodes", "PLINK"),
        queue=search("queue", "PLINK"),
        walltime=search("walltime", "PLINK"),
    envmodules:
        config["environment"]["envmodules"]["plink-2"],
    params:
        prefix="ALL_{location}",
        subsets_list=lambda wildcards: ' '.join(clusters)
    input:
        ["results/REFERENCE/cluster_%s.txt" % (cluster) for cluster in ["SUPER", "SUB"]],
        vcf="results/TRIM/ALL_{location}.vcf.gz",
    output:
        [
            expand(
                [
                    "results/FINAL/%s/ALL_{{location}}.%s.{extension}"
                    % (cluster, population)
                    for population in list(samples[cluster].unique())
                ],
                extension=finalExtensions,
            )
            for cluster in clusters
        ],
    shell:
        """
        for CLUSTER in {params.subsets_list}
        do
            echo -e "\n--- LOG SECTION START | Plink-2 'All freq' ---" 1>&2
            plink2 --allow-extra-chr --vcf {input.vcf} --freq counts --export vcf-4.2 bgz --out results/FINAL/$CLUSTER/{params.prefix}
            echo -e "--- LOG SECTION END | Plink-2 'All freq' ---\n" 1>&2
            
            echo -e "\n--- LOG SECTION START | Plink-2 'Subset freq' ---" 1>&2
            plink2 --allow-extra-chr --vcf {input.vcf} --pheno iid-only results/REFERENCE/cluster_$CLUSTER.txt --loop-cats $CLUSTER --freq counts --missing --hardy midp --out results/FINAL/$CLUSTER/{params.prefix}
            echo -e "--- LOG SECTION END | Plink-2 'Subset freq' ---\n" 1>&2
        done
        """
    
# group: VALIDATE
rule tabix:
    """
    Generate tabix-index.
    """
    log: "_logs/{operation}/{output}.log"
    benchmark: "_benchmarks/{operation}/{output}.benchmark"
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE")
    envmodules:
        config["environment"]["envmodules"]["bcftools"],
    input:
        "results/{operation}/{output}.vcf.gz"
    output:
        "results/{operation}/{output}.vcf.gz.tbi"
    shell:
        """
        tabix -p vcf {input}
        """

rule all:
    """
    Catch-all rule to trigger auto-run of all processes. This process will be fired automatically in absence of explicit process name given by cli-argument.
    """
    default_target: True
    log:
        "logs/ALL/ALL.log",
    input:
        VCFValidation.rules.all.input,
        PopulationStructure.rules.all.input,
        [
            expand(
                [
                    "results/FINAL/%s/ALL_{location}.%s.{extension}"
                    % (cluster, population)
                    for population in list(samples[cluster].unique())
                ],
                extension=finalExtensions,
                location=locations["location_name"],
            )
            for cluster in clusters
        ]
