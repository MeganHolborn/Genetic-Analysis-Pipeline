from pandas import read_csv
from os.path import join
from snakemake.utils import validate
from snakemake.utils import min_version

__author__ = "Graeme Ford"
__credits__ = [
    "Graeme Ford",
    "Prof. Michael S. Pepper",
    "Prof. Fourie Joubert",
    "Antionette Colic",
    "Fatima Barmania",
    "Sarah Turner",
    "Megan Ryder",
]
__version__ = "1.0.0"
__maintainer__ = "Graeme Ford"
__email__ = "graeme.ford@tuks.co.za"
__status__ = "Development"

# Enforce version check
min_version("6.0")


# DEFINE CONFIG FILE FOR SNAKEMAKE:
configfile: join("config", "config.json")


# validate(config, join("..", "config", ".schema", "config.schema.json"))


# pepfile: join("config", "pep.yaml")


# pepschema: join("..", "config", ".schema", "pep.schema.yaml")

locations = read_csv(join("input", "locations.csv"), header=0)
samples = read_csv(join("input", "samples.csv"), header=0)
datasets = read_csv(join("input", "datasets.csv"), header=0)
transcripts = read_csv(join("input", "transcripts.csv"), header=0)


# DEFINE CONTEXT-VARIABLES:
finalExtensions = ["acount", "hardy", "smiss", "vmiss"]  # "prune.in", "prune.out",
clusters = set([cluster for cluster in samples.keys() if cluster not in ["sample_name", "dataset"]])
bExtensions = ["bed", "bim", "fam"]
tExtensions = ["map", "ped"]


include: "rules/common.py"


# BEGIN DEFINING RULES:
rule all:
    """
    Catch-all rule to trigger auto-run of all processes. This process will be fired automatically in absence of explicit process name given by cli-argument.
    """
    log:
        "logs/ALL/ALL.log",
    input:
        [
            expand(
                [
                    "results/FINAL/%s/ALL_{location}.%s.{extension}"
                    % (cluster, population)
                    for population in list(samples[cluster].unique())
                ],
                extension=finalExtensions,
                location=locations["location_name"],
            )
            for cluster in clusters
        ],
        # expand(
        #     [
        #         "results/ADMIXTURE/%s.%s.{ext}"
        #         % (cluster, len(samples[cluster].unique()))
        #         for cluster in clusters
        #     ],
        #     cluster=clusters,
        #     ext=["bed", "bim", "fam"],
        # ),
        expand(
            [
                "results/FINAL/Admixture/%s.%s.{mode}"
                % (cluster, len(samples[cluster].unique()))
                for cluster in clusters
            ],
            mode=["P", "Q"],
        ),
        # "results/FINAL/Admixture/EIGENSOFT.pca",
        # "results/FINAL/Admixture/EIGENSOFT.plot",
        # "results/FINAL/Admixture/EIGENSOFT.eval",
        # "results/FINAL/Admixture/EIGENSOFT.log",


rule VALIDATE:
    """
    Perform validation of VCF format as well as REF alleles and strip out INFO tags.
    """
    log:
        "logs/VALIDATE/{sample}.log",
    input:
        lambda wildcards: datasets.loc[datasets["dataset_name"] == wildcards.sample, "file"].item()
    output:
        "results/PREP/{sample}.vcf.gz",
    params:
        memory=search("memory", "VALIDATE"),
    resources:
        cpus=search("cores", "VALIDATE"),
        nodes=search("nodes", "VALIDATE"),
        queue=search("queue", "VALIDATE"),
        walltime=search("walltime", "VALIDATE"),
    run:
        # Remove variant types we cant yet analyse:
        shell(
            "module load gatk-4.2.2.0; gatk SelectVariants  -V {input} --select-type-to-include SNP --select-type-to-include INDEL --select-type-to-exclude MIXED --select-type-to-exclude MNP --select-type-to-exclude SYMBOLIC --exclude-filtered -O results/PREP/{wildcards.sample}_FILTERED.vcf.gz"
        ),
        # Strip out INFO tags:
        shell(
            "module load bcftools-1.7; bcftools annotate -x INFO -O z -o results/PREP/{wildcards.sample}_NO_INFO.vcf.gz results/PREP/{wildcards.sample}_FILTERED.vcf.gz"
        ),
        # Regenerate and verify the VCF header:
        shell(
            "module load picard-2.17.11; java -Xmx{params.memory} -jar $PICARD FixVcfHeader I=results/PREP/{wildcards.sample}_NO_INFO.vcf.gz O={output}"
        )


rule LIFTOVER:
    """
    Lift Variants onto same Reference build. Otherwise we cant merge them or analyse them in context of each other.
    """
    log:
        "logs/LIFTOVER/{sample}.log",
    input:
        "results/PREP/{sample}.vcf.gz",
    output:
        "results/LIFTOVER/{sample}.vcf.gz",
    params:
        prefix=lambda wildcards: "results/LIFTOVER/{sample}_LIFTED".format(
            sample=wildcards.sample
        ),
        exclusionList=(
            lambda wildcards: "results/LIFTOVER/{sample}_EXCLUDE.dat".format(
                sample=wildcards.sample
            )
        ),
        chainFile=join("resources", "hg19ToHg38.over.chain"),
        LiftOver=join("resources", "liftOverPlink.py"),
        rmBadLifts=join("resources", "rmBadLifts.py"),
        ref=join(
            *next(
                i["file_path"]
                for i in config["reference-genomes"]
            if i["version"] == "GRCh38"
                ),
        ),
        mem=f'-Xmx{search("memory", "LIFTOVER")} ',
    resources:
        cpus=search("cores", "LIFTOVER"),
        nodes=search("nodes", "LIFTOVER"),
        queue=search("queue", "LIFTOVER"),
        walltime=search("walltime", "LIFTOVER"),
    conda:
        join("envs", "snakemake.yml")
    script:
        join("scripts", "00 - LIFTOVER.py")


rule ALL_COLLATE:
    """
    Collate Datasets together into 1 psudo-dataset for downstream analysis.
    """
    log:
        "logs/COLLATE/ALL.log",
    input:
        expand("results/LIFTOVER/{sample}.vcf.gz", sample=datasets["dataset_name"]),
    output:
        "results/COLLATE/ALL.vcf.gz",
        "results/COLLATE/ALL.vcf.gz.tbi",
    params:
        ref=lambda wildcards: join(
        *next(
            i["file_path"]
            for i in config["reference-genomes"]
        if i["version"] == "GRCh38"
            ),
        ),
    resources:
        cpus=search("cores", "COLLATE"),
        nodes=search("nodes", "COLLATE"),
        queue=search("queue", "COLLATE"),
        walltime=search("walltime", "COLLATE"),
    run:
        if datasets["dataset_name"].size > 1:
            shell(
                "module load bcftools-1.7; bcftools merge -l results/LIFTOVER/merge.list -O z -o results/COLLATE/ALL_PRE.vcf.gz"
            ),
            shell("module load samtools-1.7; tabix -p vcf results/COLLATE/ALL_PRE.vcf.gz"),
            shell(
                "module load plink-2; plink2 --vcf results/COLLATE/ALL_PRE.vcf.gz --fa {params.ref} --ref-from-fa force --allow-extra-chr --export vcf bgz --out results/COLLATE/ALL_REF"
            ),
            shell(
                "module load plink-2; plink2 --vcf results/COLLATE/ALL_REF.vcf.gz --allow-extra-chr --output-chr chr26 --chr 1-22 --export vcf-4.2 bgz --out results/COLLATE/ALL"
            ),
            shell("module load samtools-1.7; tabix -p vcf results/COLLATE/ALL.vcf.gz"),
        elif datasets["dataset_name"].size < 1:
            pass
        else:
            shell(f"cp results/LIFTOVER/{datasets['dataset_name'][0]}.vcf.gz results/COLLATE/ALL.vcf.gz"),
            shell(f"cp results/LIFTOVER/{datasets['dataset_name'][0]}.vcf.gz.tbi results/COLLATE/ALL.vcf.gz.tbi"),


rule ANNOTATE:
    """
    Annotate rsID's in psudo-dataset to facilitate down-stream analysis.
    """
    log:
        "logs/ANNOTATE/ALL.log",
    input:
        "results/COLLATE/ALL.vcf.gz",
    output:
        "results/ANNOTATE/ALL.vcf.gz",
    resources:
        cpus=search("cores", "ANNOTATE"),
        nodes=search("nodes", "ANNOTATE"),
        queue=search("queue", "ANNOTATE"),
        walltime=search("walltime", "ANNOTATE"),
    # ToDo: Add refGenome and dbSNP links
    run:
        shell(
            "module load bcftools-1.7; bcftools annotate -c ID  -a /nlustre/data/gatk_resource_bundle/hg38/dbsnp_146.hg38.vcf.gz -O z -o results/ANNOTATE/ALL_ANNOTATED.vcf.gz {input}"
        ),
        shell(
            "module load plink-2; plink2 --allow-extra-chr --vcf results/ANNOTATE/ALL_ANNOTATED.vcf.gz --export vcf-4.2 bgz --out results/ANNOTATE/ALL"
        ),


rule ADMIXTURE:
    """
    Perform Admixture analysis on the large psudo-dataset (Requires 100 000 minimum variants to distinguish sub-populations and 10 000 to distinguish super-populations.)
    """
    log:
        "logs/ADMIXTURE/{cluster}.{estimation}.log",
    input:
        "results/ANNOTATE/ALL.vcf.gz",
    output:
        expand(
            "results/ADMIXTURE/{{cluster}}.{{estimation}}.{ext}",
            ext=["bed", "bim", "fam"],
        ),
        # "results/FINAL/Admixture/EIGENSOFT.pca",
        # "results/FINAL/Admixture/EIGENSOFT.plot",
        # "results/FINAL/Admixture/EIGENSOFT.eval",
        # "results/FINAL/Admixture/EIGENSOFT.log",
        expand(
            "results/FINAL/Admixture/{{cluster}}.{{estimation}}.{mode}",
            mode=["P", "Q"],
        ),
    resources:
        cpus=search("cores", "ADMIXTURE"),
        nodes=search("nodes", "ADMIXTURE"),
        queue=search("queue", "ADMIXTURE"),
        walltime=search("walltime", "ADMIXTURE"),
    run:
        shell(
            "module load bcftools-1.7; bcftools view -O z -o results/ADMIXTURE/FILTERED.vcf.gz -m2 -M2 -v snps {input}"
        ),
        shell(
            "module load plink-2; plink2 --allow-extra-chr --vcf results/ADMIXTURE/FILTERED.vcf.gz --thin-count 200000 --set-missing-var-ids @_# --make-bed --out results/ADMIXTURE/{wildcards.cluster}.{wildcards.estimation}"
        ),
        shell(
            "module load admixture-1.3.0; admixture results/ADMIXTURE/{wildcards.cluster}.{wildcards.estimation}.bed {wildcards.estimation}"
        ),
        shell(
            "mv {wildcards.cluster}.{wildcards.estimation}.{wildcards.estimation}.P results/FINAL/Admixture/{wildcards.cluster}.{wildcards.estimation}.P"
        ),
        shell(
            "mv {wildcards.cluster}.{wildcards.estimation}.{wildcards.estimation}.Q results/FINAL/Admixture/{wildcards.cluster}.{wildcards.estimation}.Q"
        ),
        # shell("mv {params.path}THINNED.bim {params.path}.pedsnp"),
        # shell("mv {params.path}THINNED.fam {params.path}.pedind"),
        # shell("module load eigensoft; smartpca -i {params.path}THINNED.bed -a {params.path}THINNED.pedsnp -b {params.path}THINNED.pedind -o {params.finalPath}/EIGENSOFT.pca -p {params.finalPath}/EIGENSOFT.plot -e {params.finalPath}/EIGENSOFT.eval -l {params.finalPath}/EIGENSOFT.log")



rule TRIM_AND_NAME:
    """
    Trim the whole-genome psudo-datasets down to several regions of interest for Variant analysis and Variant effect prediction.
    """
    log:
        "logs/TRIM/{location}.log",
    input:
        "results/ANNOTATE/ALL.vcf.gz",
    output:
        "results/TRIM/ALL_{location}_TRIMMED.vcf.gz",
    params:
        fromBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "start"].item(),
        toBP=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "stop"].item(),
        chr=lambda wildcards: locations.loc[locations["location_name"] == wildcards.location, "chromosome"].item(),
    resources:
        cpus=search("cores", "TRIM_AND_NAME"),
        nodes=search("nodes", "TRIM_AND_NAME"),
        queue=search("queue", "TRIM_AND_NAME"),
        walltime=search("walltime", "TRIM_AND_NAME"),
    run:
        shell(
            "module load plink-2; plink2 --allow-extra-chr --vcf {input} --from-bp {params.fromBP} --to-bp {params.toBP} --chr {params.chr} --export vcf-4.2 bgz --out results/TRIM/ALL_{wildcards.location}_TRIMMED"
        ),


rule FILTER:
    """
    Filter out individuals missing 100% of their variant information (Safety Check).
    """
    log:
        "logs/FILTER/{location}.log",
    input:
        "results/TRIM/ALL_{location}_TRIMMED.vcf.gz",
    output:
        "results/FINAL/ALL_{location}.vcf.gz",
        "results/FILTER/ALL_{location}_FILTERED.log",
    resources:
        cpus=search("cores", "FILTER"),
        nodes=search("nodes", "FILTER"),
        queue=search("queue", "FILTER"),
        walltime=search("walltime", "FILTER"),
    run:
        shell(
            "module load plink-2; plink2 --allow-extra-chr --vcf {input} --mind 1 --output-chr chr26 --export vcf-4.2 bgz --out results/FILTER/ALL_{wildcards.location}_FILTERED"
        ),
        shell(
            "module load bcftools-1.7; bcftools norm -m - -o results/FINAL/ALL_{wildcards.location}.vcf.gz -O z results/FILTER/ALL_{wildcards.location}_FILTERED.vcf.gz"
        )


rule TRANSPILE_CLUSTERS:
    """
    Convert Cluster information given in the config file into PLINK-2.0 suitable format.
    """
    log:
        "logs/TRANSPILE/{cluster}.log",
    output:
        "results/REFERENCE/cluster_{cluster}.txt",
    resources:
        cpus=search("cores", "TRANSPILE_CLUSTERS"),
        nodes=search("nodes", "TRANSPILE_CLUSTERS"),
        queue=search("queue", "TRANSPILE_CLUSTERS"),
        walltime=search("walltime", "TRANSPILE_CLUSTERS"),
    conda:
        join("envs", "snakemake.yml")
    script:
        join("scripts", "01 - TRANSPILE_CLUSTERS.py")


rule PLINK:
    """
    Perform Frequency analysis on super populations.
    """
    log:
        "logs/PLINK/{location}.log",
    input:
        ["results/REFERENCE/cluster_%s.txt" % (cluster) for cluster in ["SUPER", "SUB"]],
        vcf="results/FINAL/ALL_{location}.vcf.gz",
    output:
        [
            expand(
                [
                    "results/FINAL/%s/ALL_{{location}}.%s.{extension}"
                    % (cluster, population)
                    for population in list(samples[cluster].unique())
                ],
                extension=finalExtensions,
            )
            for cluster in clusters
        ],
    params:
        prefix="ALL_{location}",
    resources:
        cpus=search("cores", "PLINK"),
        nodes=search("nodes", "PLINK"),
        queue=search("queue", "PLINK"),
        walltime=search("walltime", "PLINK"),
    run:
        for cluster in clusters:
            shell(
                "module load plink-2; plink2 --allow-extra-chr --vcf {input.vcf} --freq counts --export vcf-4.2 bgz --out results/FINAL/{cluster}/{params.prefix}"
            ),
            shell(
                "module load plink-2; plink2 --allow-extra-chr --vcf {input.vcf} --pheno iid-only results/REFERENCE/cluster_{cluster}.txt --loop-cats {cluster} --freq counts --missing --hardy midp --out results/FINAL/{cluster}/{params.prefix}"
            )


# --indep-pairwise 50 5 .05 removed since this dataset has redundant variant names.


# Add in VEP API calls
# rule CALCULATIONS:
#     """
#     Perform Variant Effect prediction with the E! Ensembl VEP tool API.
#     """
#     log:
#         "logs/VEP/{location}.log",
#     input:
#         vcf="results/FILTER/ALL_{location}_FILTERED.vcf.gz",
#     output:
#         expand("results/FINAL/{cluster}-{location}.xlsx"),
#     params:
#         transcript_id=lambda wildcards: config["locations"][wildcards.location][
#             "GRCh38"
#         ]["transcript_id"],
#     resources:
#         cpus=10,
#         nodes=1,
#         queue="normal",
#         walltime="30:00:00",
#     script:
#         "scripts/02 - CALCULATIONS.py"
# rule COMPILE_EXCEL:
#     """
#     Compile all calculations into final excel for distribution and analysis.
#     """
#     input:
#         vcf="results/FILTER/ALL_{location}_FILTERED.vcf.gz",
#     output:
#         expand("results/FINAL/{cluster}-{location}.xlsx"),
#     resources:
#         cpus=10,
#         nodes=1,
#         queue="normal",
#         walltime="30:00:00",
#     script:
#         "scripts/03 - MERGE.py"
